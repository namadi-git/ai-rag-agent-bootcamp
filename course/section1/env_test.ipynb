{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a5a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI, OpenAIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727f01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f9745",
   "metadata": {},
   "source": [
    "## Interact with api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba756d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the moon break up with the sun?\n",
      "\n",
      "Because it needed some space! üåô‚ú®\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI, OpenAIError\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "try:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=\"Tell a joke about the moon\"\n",
    "    )\n",
    "    print(response.output_text)\n",
    "except OpenAIError as e:\n",
    "    print(f\"OpenAI API error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9a9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke about Alexey\"}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=messages\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bcd6ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Alexey bring a ladder to the bar?\n",
      "\n",
      "Because he heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11e03de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_053fd613543851bd0068e722a7a2088193a4f3912b078484be\",\n",
      "  \"created_at\": 1759978151.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_053fd613543851bd0068e722a8898c8193bebc2fbd2149325d\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"Why did Alexey bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 14,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 22,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 36\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"billing\": {\n",
      "    \"payer\": \"developer\"\n",
      "  },\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69846d97",
   "metadata": {},
   "source": [
    "What is interesting here:\n",
    "\n",
    "output - everything that the LLM returned\n",
    "\n",
    "usage - how many tokens we used (also - how much we paid)\n",
    "\n",
    "Getting the text:\n",
    "\n",
    "response.output[0].content[0].text\n",
    "\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac57db",
   "metadata": {},
   "source": [
    "## Stream response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2db0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_05fa3cc314f49d6d0068e723437320819492c912adfed90937', created_at=1759978307.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_05fa3cc314f49d6d0068e723437320819492c912adfed90937', created_at=1759978307.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Why', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='q5I4SsgQqmaOc')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' did', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='AIvBSrjWJtbf')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' Alex', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=6, type='response.output_text.delta', obfuscation='7vZxWdOwd7T')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='ey', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=7, type='response.output_text.delta', obfuscation='WOYtWhemOK2v1v')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' bring', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=8, type='response.output_text.delta', obfuscation='NwzybE0eWs')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' a', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=9, type='response.output_text.delta', obfuscation='HaQhYBM2Y0B1YD')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' ladder', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=10, type='response.output_text.delta', obfuscation='3gVbIPpId')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' to', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=11, type='response.output_text.delta', obfuscation='fMMojRTlRP8Oz')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' the', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=12, type='response.output_text.delta', obfuscation='74uXLW2IFL8a')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' bar', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=13, type='response.output_text.delta', obfuscation='9DKIbPI35zEU')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='?\\n\\n', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=14, type='response.output_text.delta', obfuscation='pxT0uT4Ju69LK')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Because', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=15, type='response.output_text.delta', obfuscation='zsFAHy6xn')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' he', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=16, type='response.output_text.delta', obfuscation='cYxq4md6RWgTx')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' heard', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=17, type='response.output_text.delta', obfuscation='VSItmG1CVe')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' the', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=18, type='response.output_text.delta', obfuscation='bmmMq52jaO6c')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' drinks', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=19, type='response.output_text.delta', obfuscation='jkDFZpyHO')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' were', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=20, type='response.output_text.delta', obfuscation='hHAP6QZ9163')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' on', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=21, type='response.output_text.delta', obfuscation='gm9y4pGyugAMO')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' the', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=22, type='response.output_text.delta', obfuscation='CqbxBrkvS68K')\n",
      "ResponseTextDeltaEvent(content_index=0, delta=' house', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=23, type='response.output_text.delta', obfuscation='j0bL6HBWyc')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='!', item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=24, type='response.output_text.delta', obfuscation='eRx6M3lI1BWJY5s')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', logprobs=[], output_index=0, sequence_number=25, text='Why did Alexey bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', output_index=0, part=ResponseOutputText(annotations=[], text='Why did Alexey bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!', type='output_text', logprobs=[]), sequence_number=26, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', content=[ResponseOutputText(annotations=[], text='Why did Alexey bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=27, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_05fa3cc314f49d6d0068e723437320819492c912adfed90937', created_at=1759978307.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_05fa3cc314f49d6d0068e72344b9948194ac97782e2608afd2', content=[ResponseOutputText(annotations=[], text='Why did Alexey bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=14, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=36), user=None, store=True), sequence_number=28, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "stream = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2345959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Alexey bring a ladder to the bar?\n",
      "\n",
      "Because he heard the drinks were on the house!"
     ]
    }
   ],
   "source": [
    "stream = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=messages,\n",
    "    stream=True\n",
    ")\n",
    "for event in stream:\n",
    "    if hasattr(event, 'delta'):\n",
    "        print(event.delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58e769",
   "metadata": {},
   "source": [
    "## System and User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e809f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! But first, what's your name?\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You're an assistant that can make jokes. Always find out the name of\n",
    "the person to make the jokes personalized. Once you know the name,\n",
    "make the joke about them.\n",
    "\"\"\".strip()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke\"}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=messages\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0cb602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great to meet you, Nelson! Here‚Äôs a joke just for you:\n",
      "\n",
      "Why did Nelson bring a ladder to the bar?\n",
      "\n",
      "Because he heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "messages.append(response.output[0])\n",
    "messages.append({\"role\": \"user\", \"content\": \"Nelson\"})\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=messages\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c15ee1",
   "metadata": {},
   "source": [
    "##  Save and continue chat history using ToyAi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e0671",
   "metadata": {},
   "source": [
    "It will keep track of the previous conversation and send it back to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58889336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c78675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I can't check the date directly, but you can usually find it on your device! By the way, what's your name? I‚Äôd love to make a joke just for you!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It looks like you‚Äôre testing me! Does that mean I need to pass a ‚Äúfunny‚Äù test? If so, what's your name so I can ace it with a personalized joke?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Sure! But first, what's your name so I can personalize it?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Thanks, Nelson! Here‚Äôs a joke for you:</p>\n",
       "<p>Why did the weather report break up with Nelson?</p>\n",
       "<p>Because it found someone who promised sunny days and no chance of &quot;cloudy with a chance of meatballs!&quot; üå¶Ô∏èüòÑ</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Looks like you didn‚Äôt have a response! If you want to hear another joke or talk about something else, just let me know, Nelson!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>No worries, Nelson! If you have any questions or if there‚Äôs something you'd like to chat about, just let me know! I‚Äôm here to help (and joke around)!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Got it, Nelson! If you ever want to chat or hear a joke again, just drop by. Have a great day! üòÑ</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      7\u001b[39m llm_client = OpenAIClient(\n\u001b[32m      8\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m     client=client\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m runner = OpenAIResponsesRunner(\n\u001b[32m     13\u001b[39m     tools=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     14\u001b[39m     developer_prompt=system_prompt,\n\u001b[32m     15\u001b[39m     chat_interface=IPythonChatInterface(),\n\u001b[32m     16\u001b[39m     llm_client=llm_client\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nelso\\Documents\\github\\ai-rag-agent-bootcamp\\course\\.venv\\Lib\\site-packages\\toyaikit\\chat\\runners.py:159\u001b[39m, in \u001b[36mOpenAIResponsesRunner.run\u001b[39m\u001b[34m(self, previous_messages, stop_criteria)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Chat loop\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     question = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_interface\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m question.lower() == \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    161\u001b[39m         \u001b[38;5;28mself\u001b[39m.chat_interface.display(\u001b[33m\"\u001b[39m\u001b[33mChat ended.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nelso\\Documents\\github\\ai-rag-agent-bootcamp\\course\\.venv\\Lib\\site-packages\\toyaikit\\chat\\interface.py:59\u001b[39m, in \u001b[36mIPythonChatInterface.input\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minput\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     question = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m question.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nelso\\Documents\\github\\ai-rag-agent-bootcamp\\course\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nelso\\Documents\\github\\ai-rag-agent-bootcamp\\course\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "you're an assistant that can make jokes. Always find out the name of\n",
    "the person to make the jokes personalized. Once you know the name,\n",
    "make the joke about them.\n",
    "\"\"\".strip()\n",
    "\n",
    "llm_client = OpenAIClient(\n",
    "    model='gpt-4o-mini',\n",
    "    client=client\n",
    ")\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=None,\n",
    "    developer_prompt=system_prompt,\n",
    "    chat_interface=IPythonChatInterface(),\n",
    "    llm_client=llm_client\n",
    ")\n",
    "\n",
    "runner.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ebfd1",
   "metadata": {},
   "source": [
    "## Conversational Chat using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d0b44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nelso\\AppData\\Local\\Temp\\ipykernel_30204\\2409486733.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n",
      "C:\\Users\\nelso\\AppData\\Local\\Temp\\ipykernel_30204\\2409486733.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(llm=llm, memory=memory, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "# Keeps the full history in RAM for this Python process\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "chain = ConversationChain(llm=llm, memory=memory, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "077d5631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Nelson! It's great to meet you! NBA fantasy is such an exciting way to engage with the league. Do you have a favorite team or player that you like to build your fantasy roster around? Or maybe you have some strategies you swear by for drafting and managing your team?\n",
      "You mentioned that your name is Nelson! It's nice to meet you, Nelson.\n",
      "You mentioned that you love NBA fantasy! It's a fun way to get involved with the league and track your favorite players. Do you have any specific players or strategies you like to focus on in your fantasy leagues?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chain.predict(input=\"Hi, I'm Nelson. I love NBA fantasy.\"))\n",
    "print(chain.predict(input=\"What did I say my name is?\"))\n",
    "print(chain.predict(input=\"And what hobby did I mention?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4d306",
   "metadata": {},
   "source": [
    "## Conversational Chat using LECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9723797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1Ô∏è‚É£ Define the LLM and prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful and concise assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "chain = prompt | llm\n",
    "\n",
    "# 2Ô∏è‚É£ In-memory history store keyed by session_id\n",
    "store: Dict[str, BaseChatMessageHistory] = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 3Ô∏è‚É£ Wrap the chain with memory\n",
    "chat_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Run a conversation\n",
    "cfg = {\"configurable\": {\"session_id\": \"user_123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0324b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Nelson! It's great to meet a fellow NBA fantasy enthusiast. How can I assist you with your fantasy basketball needs?\n"
     ]
    }
   ],
   "source": [
    "resp1 = chat_with_memory.invoke({\"input\": \"Hi, I'm Nelson from Houston who loves NBA fantasy.\"}, cfg)\n",
    "print(resp1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10129bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You mentioned that you love NBA fantasy.\n"
     ]
    }
   ],
   "source": [
    "resp2 = chat_with_memory.invoke({\"input\": \"What hobby did I say I like?\"}, cfg)\n",
    "print(resp2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a20d1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, I'm Nelson from Houston who loves NBA fantasy.\n",
      "Assistant: Hi Nelson! It's great to meet a fellow NBA fantasy enthusiast. How can I assist you with your fantasy basketball needs?\n",
      "User: What hobby did I say I like?\n",
      "Assistant: You mentioned that you love NBA fantasy.\n"
     ]
    }
   ],
   "source": [
    "# Optional: see stored messages\n",
    "history = get_session_history(\"user_123\")\n",
    "for m in history.messages:\n",
    "    who = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
    "    print(f\"{who}: {m.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6dd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
