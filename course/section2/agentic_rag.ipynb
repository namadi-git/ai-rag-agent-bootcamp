{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ee91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "from typing import List, Dict, Any\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a6876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x17295965190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf35f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc7d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b30cd1",
   "metadata": {},
   "source": [
    "### implement the agentic loop using packages (toyai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d4ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback\n",
    "from toyaikit.tools import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06e313c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tools:\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a512f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the instructions for our agent:\n",
    "instructions = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8773e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runner configuration:\n",
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=instructions,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ef5baee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Installing Apache Kafka involves several steps, and it can vary based on your operating system. I can look up specific installation instructions to ensure you get the most accurate and up-to-date information. This will include details for different platforms, like Windows, Mac, and Linux.</p>\n",
       "<p>Let's find that information!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to install Kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to install Kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_4CDH426BEp8whqUaiYTeFkjx', 'output': '[\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It looks like the search results didn't return direct installation instructions for Kafka. However, I can provide a general overview of how to install Apache Kafka based on typical steps:</p>\n",
       "<h3>Installation Steps for Apache Kafka</h3>\n",
       "<ol>\n",
       "<li><p><strong>Prerequisites:</strong></p>\n",
       "<ul>\n",
       "<li>Ensure you have Java installed (JDK version 8 or higher).</li>\n",
       "<li>You may want to have a package manager depending on your OS (e.g., <code>apt</code> for Debian/Ubuntu, <code>brew</code> for macOS).</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Download Kafka:</strong></p>\n",
       "<ul>\n",
       "<li>Go to the <a href=\"https://kafka.apache.org/downloads\">Apache Kafka downloads page</a>.</li>\n",
       "<li>Choose a release version and download the tar file (e.g., <code>kafka_2.12-3.1.0.tgz</code>).</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Extract the Archive:</strong></p>\n",
       "<pre><code class=\"language-bash\">tar -xzf kafka_2.12-3.1.0.tgz\n",
       "cd kafka_2.12-3.1.0\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Zookeeper:</strong>\n",
       "Kafka uses Zookeeper to manage distributed brokers. Open a new terminal window and run:</p>\n",
       "<pre><code class=\"language-bash\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Kafka Server:</strong>\n",
       "In another terminal window, start the Kafka server:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create a Topic:</strong>\n",
       "You can create a topic to send messages to:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-topics.sh --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Send Messages (Optional):</strong>\n",
       "You can send messages to the topic with:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Consume Messages (Optional):</strong>\n",
       "In another terminal, consume the messages:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Additional Resources</h3>\n",
       "<ul>\n",
       "<li>For more detailed instructions or advanced configurations, check the <a href=\"https://kafka.apache.org/documentation/\">Kafka documentation</a>.</li>\n",
       "</ul>\n",
       "<p>If you have any specific operating system or version you'd like help with, feel free to let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's run the loop method. We will also use DisplayingRunnerCallback for displaying the results:\n",
    "callback = DisplayingRunnerCallback(chat_interface)\n",
    "\n",
    "question = 'how do I install kafka'\n",
    "loop_result = runner.loop(prompt=question, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c952e3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<toyaikit.chat.runners.DisplayingRunnerCallback at 0x17298b90770>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DisplayingRunnerCallback(chat_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb45459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'developer',\n",
       "  'content': \"You're a course teaching assistant. \\nYou're given a question from a course student and your task is to answer it.\\n\\nIf you want to look up the answer, explain why before making the call\"},\n",
       " {'role': 'user', 'content': 'how do I install kafka'},\n",
       " ResponseOutputMessage(id='msg_0375f5e1095daa8d0068f41edffa588196a287d15b64817a38', content=[ResponseOutputText(annotations=[], text=\"Installing Apache Kafka involves several steps, and it can vary based on your operating system. I can look up specific installation instructions to ensure you get the most accurate and up-to-date information. This will include details for different platforms, like Windows, Mac, and Linux.\\n\\nLet's find that information!\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"how to install Kafka\"}', call_id='call_4CDH426BEp8whqUaiYTeFkjx', name='search', type='function_call', id='fc_0375f5e1095daa8d0068f41ee0eac88196b5d91ed201396e96', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_4CDH426BEp8whqUaiYTeFkjx',\n",
       "  'output': '[\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'},\n",
       " ResponseOutputMessage(id='msg_0375f5e1095daa8d0068f41ee203188196b4550479a5a87d28', content=[ResponseOutputText(annotations=[], text=\"It looks like the search results didn't return direct installation instructions for Kafka. However, I can provide a general overview of how to install Apache Kafka based on typical steps:\\n\\n### Installation Steps for Apache Kafka\\n\\n1. **Prerequisites:**\\n   - Ensure you have Java installed (JDK version 8 or higher).\\n   - You may want to have a package manager depending on your OS (e.g., `apt` for Debian/Ubuntu, `brew` for macOS).\\n\\n2. **Download Kafka:**\\n   - Go to the [Apache Kafka downloads page](https://kafka.apache.org/downloads).\\n   - Choose a release version and download the tar file (e.g., `kafka_2.12-3.1.0.tgz`).\\n\\n3. **Extract the Archive:**\\n   ```bash\\n   tar -xzf kafka_2.12-3.1.0.tgz\\n   cd kafka_2.12-3.1.0\\n   ```\\n\\n4. **Start Zookeeper:**\\n   Kafka uses Zookeeper to manage distributed brokers. Open a new terminal window and run:\\n   ```bash\\n   bin/zookeeper-server-start.sh config/zookeeper.properties\\n   ```\\n\\n5. **Start Kafka Server:**\\n   In another terminal window, start the Kafka server:\\n   ```bash\\n   bin/kafka-server-start.sh config/server.properties\\n   ```\\n\\n6. **Create a Topic:**\\n   You can create a topic to send messages to:\\n   ```bash\\n   bin/kafka-topics.sh --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\\n   ```\\n\\n7. **Send Messages (Optional):**\\n   You can send messages to the topic with:\\n   ```bash\\n   bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092\\n   ```\\n\\n8. **Consume Messages (Optional):**\\n   In another terminal, consume the messages:\\n   ```bash\\n   bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092\\n   ```\\n\\n### Additional Resources\\n- For more detailed instructions or advanced configurations, check the [Kafka documentation](https://kafka.apache.org/documentation/).\\n\\nIf you have any specific operating system or version you'd like help with, feel free to let me know!\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a31df7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To run Kafka in Python, you'll typically use libraries like <code>kafka-python</code> or <code>confluent-kafka-python</code>. Here are the steps for setting this up:</p>\n",
       "<h3>Steps to Use Kafka with Python</h3>\n",
       "<ol>\n",
       "<li><p><strong>Install Kafka Library:</strong>\n",
       "You can choose either <code>kafka-python</code> or <code>confluent-kafka-python</code>. Install one of them using pip:</p>\n",
       "<pre><code class=\"language-bash\">pip install kafka-python\n",
       "</code></pre>\n",
       "<p>or for Confluent Kafka:</p>\n",
       "<pre><code class=\"language-bash\">pip install confluent-kafka\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create a Kafka Producer:</strong>\n",
       "Here's a simple example using <code>kafka-python</code>:</p>\n",
       "<pre><code class=\"language-python\">from kafka import KafkaProducer\n",
       "\n",
       "# Create a producer\n",
       "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
       "\n",
       "# Send a message\n",
       "producer.send('test', b'Hello, Kafka!')\n",
       "producer.flush()  # Ensure all messages are sent\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create a Kafka Consumer:</strong>\n",
       "Example of consuming messages:</p>\n",
       "<pre><code class=\"language-python\">from kafka import KafkaConsumer\n",
       "\n",
       "# Create a consumer\n",
       "consumer = KafkaConsumer('test', bootstrap_servers='localhost:9092', auto_offset_reset='earliest')\n",
       "\n",
       "for message in consumer:\n",
       "    print(f&quot;Received message: {message.value.decode('utf-8')}&quot;)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Run Your Code:</strong></p>\n",
       "<ul>\n",
       "<li>Ensure your Kafka server is running as described in the previous message.</li>\n",
       "<li>Run the producer script to send messages and then run the consumer script to receive them.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Notes:</h3>\n",
       "<ul>\n",
       "<li>Ensure that the Kafka server (<code>localhost:9092</code> in the examples) is correctly set up and running.</li>\n",
       "<li>Replace <code>'test'</code> with the name of your Kafka topic.</li>\n",
       "<li>You can explore further functionalities, like handling serialization and configuring different producer/consumer settings.</li>\n",
       "</ul>\n",
       "<h3>Additional Resources</h3>\n",
       "<ul>\n",
       "<li>You can refer to the <a href=\"https://kafka-python.readthedocs.io/en/master/\">kafka-python documentation</a> or <a href=\"https://docs.confluent.io/platform/current/clients/python.html\">Confluent Kafka documentation</a> for more advanced usage.</li>\n",
       "</ul>\n",
       "<p>If you want detailed examples or have a specific use case in mind, let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = 'but how do I run it in python?'\n",
    "loop_result = runner.loop(prompt=question, previous_messages=loop_result, callback=callback)\n",
    "# Note that in this case, it decided not to make any further function calls since it had sufficient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edf2393b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Using Apache can vary based on what you want to achieve (e.g., setting up a web server, configuring virtual hosts, etc.). To provide you with a comprehensive answer, I can look up common questions related to using Apache. This will help ensure I cover various aspects, such as installation, configuration, and common commands.</p>\n",
       "<p>I'll search for frequently asked questions about using Apache to provide you with the best guidance.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to use apache\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to use apache\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_KhkcDvz2UtpF4lB9zLfBGBrt', 'output': '[\\n  {\\n    \"text\": \"If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\\\nFor Windows, create a new User Variable \\\\u201cHADOOP_HOME\\\\u201d that points to your Hadoop directory. Then add \\\\u201c%HADOOP_HOME%\\\\\\\\bin\\\\u201d to the PATH variable.\\\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Hadoop - Exception in thread \\\\\"main\\\\\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Run this before SparkSession\\\\nimport os\\\\nimport sys\\\\nos.environ[\\'PYSPARK_PYTHON\\'] = sys.executable\\\\nos.environ[\\'PYSPARK_DRIVER_PYTHON\\'] = sys.executable\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Make sure your java version is 11 or 8.\\\\nCheck your version by:\\\\njava --version\\\\nCheck all your versions by:\\\\n/usr/libexec/java_home -V\\\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\\\n(or other version of 11)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"It\\'s up to you which platform and environment you use for the course.\\\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\\\nHaving this local repository on your computer will make it easy for you to access the instructors\\\\u2019 code and make pull requests (if you want to add your own notes or make changes to the course content).\\\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\\\nThis is also a great resource: https://dangitgit.com/\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"How do I use Git / GitHub for this course?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems that the search returned results related to issues with Hadoop and Spark rather than specific guidance on using Apache HTTP Server. However, I can provide a general overview of how to use Apache based on common practices.</p>\n",
       "<h3>Overview of Using Apache HTTP Server</h3>\n",
       "<ol>\n",
       "<li><p><strong>Installation</strong>:</p>\n",
       "<ul>\n",
       "<li><strong>On Linux</strong>: You can typically install Apache using the package manager.<ul>\n",
       "<li>For Debian/Ubuntu:<pre><code class=\"language-bash\">sudo apt update\n",
       "sudo apt install apache2\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>For Red Hat/CentOS:<pre><code class=\"language-bash\">sudo yum install httpd\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>On Windows</strong>: You can download Apache from <a href=\"https://www.apachelounge.com/download/\">Apache Lounge</a>.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Starting Apache</strong>:</p>\n",
       "<ul>\n",
       "<li><strong>On Linux</strong>:<pre><code class=\"language-bash\">sudo systemctl start apache2  # For Ubuntu\n",
       "sudo systemctl start httpd    # For CentOS\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><strong>On Windows</strong>: Use the command prompt to navigate to the Apache <code>bin</code> directory and run:<pre><code class=\"language-bash\">httpd -k start\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Configuration</strong>:</p>\n",
       "<ul>\n",
       "<li>Configuration files are usually found in:<ul>\n",
       "<li><strong>Linux</strong>: <code>/etc/apache2/apache2.conf</code> or <code>/etc/httpd/conf/httpd.conf</code></li>\n",
       "<li><strong>Windows</strong>: <code>C:\\Program Files\\Apache Group\\Apache2\\conf\\httpd.conf</code></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>You can edit the configuration file to set up virtual hosts, security settings, and other parameters.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Testing Your Server</strong>:</p>\n",
       "<ul>\n",
       "<li>Open a web browser and go to <code>http://localhost</code>. If Apache is running, you should see the default Apache welcome page.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Common Commands</strong>:</p>\n",
       "<ul>\n",
       "<li>To check the status of Apache:<pre><code class=\"language-bash\">sudo systemctl status apache2  # Ubuntu\n",
       "sudo systemctl status httpd    # CentOS\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>To stop or restart Apache:<pre><code class=\"language-bash\">sudo systemctl stop apache2\n",
       "sudo systemctl restart apache2\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Additional Resources</h3>\n",
       "<ul>\n",
       "<li><strong>Documentation</strong>: Check out the <a href=\"https://httpd.apache.org/docs/\">official Apache documentation</a> for detailed guides on configuration and advanced usage.</li>\n",
       "<li><strong>Learning Resources</strong>: Look for tutorials online or sites like DigitalOcean for step-by-step guides.</li>\n",
       "</ul>\n",
       "<p>If you have any specific tasks you want to achieve with Apache, please let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e4c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more tools to the agent\n",
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)\n",
    "\n",
    "\n",
    "add_entry_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f29ac3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools.add_tool(add_entry, add_entry_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "811210bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To help you effectively, I'd like to look up specific strategies or tips related to performing well in Module 1 of your course. There may be guidelines or common advice already available in the FAQ that can provide detailed insights.</p>\n",
       "<p>I’ll search for information related to succeeding in Module 1.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Module 1 success tips\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"Module 1 success tips\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_rUxiB5C9KmVgaccHywPCasz9', 'output': '[\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The search results didn’t return specific success tips for Module 1, but they did include some technical issues and their solutions related to common challenges you might encounter, particularly with tools or programming environments used in this module.</p>\n",
       "<p>To do well in Module 1, here are some general strategies you can follow:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand the Fundamentals</strong>: Make sure you grasp the basic concepts introduced in the module. Don’t hesitate to review resources or ask questions if you’re uncertain about any topic.</p>\n",
       "</li>\n",
       "<li><p><strong>Hands-On Practice</strong>: Engage in practical exercises. If there are coding tasks, work through them thoroughly. Practicing will enhance your understanding and retention of the material.</p>\n",
       "</li>\n",
       "<li><p><strong>Utilize Resources</strong>: Use any supplementary materials provided, such as readings, videos, or tutorials. They can provide different perspectives and explanations that might help you understand better.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with Peers</strong>: Collaborating with classmates can provide support and enhance learning through discussion and group work.</p>\n",
       "</li>\n",
       "<li><p><strong>Seek Help When Needed</strong>: If you encounter challenges, particularly technical ones (like installation errors), refer to the FAQs or ask instructors for assistance.</p>\n",
       "</li>\n",
       "<li><p><strong>Stay Organized</strong>: Keep your notes and resources well-organized, so you can easily refer to them when studying or working on assignments.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have specific areas within Module 1 you need help with, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"How do I do well in Module 1?\",\"an...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"question\":\"How do I do well in Module 1?\",\"answer\":\"1. **Understand the Fundamentals**: Make sure you grasp the basic concepts introduced in the module. Don’t hesitate to review resources or ask questions if you’re uncertain about any topic.\\n2. **Hands-On Practice**: Engage in practical exercises. If there are coding tasks, work through them thoroughly. Practicing will enhance your understanding and retention of the material.\\n3. **Utilize Resources**: Use any supplementary materials provided, such as readings, videos, or tutorials. They can provide different perspectives and explanations that might help you understand better.\\n4. **Engage with Peers**: Collaborating with classmates can provide support and enhance learning through discussion and group work.\\n5. **Seek Help When Needed**: If you encounter challenges, particularly technical ones (like installation errors), refer to the FAQs or ask instructors for assistance.\\n6. **Stay Organized**: Keep your notes and resources well-organized, so you can easily refer to them when studying or working on assignments.\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_wFUrtwPiiNf3nCZebsUec6Fm', 'output': 'null'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I've successfully added the guidance on how to do well in Module 1 to the FAQ. If you have any more questions or need further assistance, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Got it! If you need anything else later, just let me know. Good luck with your studies!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "runner.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec4cfd",
   "metadata": {},
   "source": [
    "## Putting Tools in One Class\n",
    "instead pf creating the tool function defintition schema use the the docstring and typehints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f93a6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the FAQ database for entries matching the given query.\n",
    "    \n",
    "        Args:\n",
    "            query (str): Search query text to look up in the course FAQ.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "        \"\"\"\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "    \n",
    "        results = self.index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def add_entry(self, question: str, answer: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a new entry to the FAQ database.\n",
    "    \n",
    "        Args:\n",
    "            question (str): The question to be added to the FAQ database.\n",
    "            answer (str): The corresponding answer to the question.\n",
    "        \"\"\"\n",
    "        doc = {\n",
    "            'question': question,\n",
    "            'text': answer,\n",
    "            'section': 'user added',\n",
    "            'course': 'data-engineering-zoomcamp'\n",
    "        }\n",
    "        self.index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc080a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools(index) # create an instance of the SearchTools class\n",
    "agent_tools = Tools() # create a Tools container\n",
    "agent_tools.add_tools(search_tools) # add the tools from the SearchTools instance, no need to define the schema manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9892543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add a new entry to the FAQ database.\\n\\nArgs:\\n    question (str): The question to be added to the FAQ database.\\n    answer (str): The corresponding answer to the question.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'question parameter'},\n",
       "    'answer': {'type': 'string', 'description': 'answer parameter'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database for entries matching the given query.\\n\\nArgs:\\n    query (str): Search query text to look up in the course FAQ.\\n\\nReturns:\\n    List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'query parameter'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cde838d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To provide an accurate answer to this question, I need to look up the course description or FAQ. This information will detail the focus and objectives of the class, allowing me to give you a clear understanding of what to expect.</p>\n",
       "<p>I'll search the FAQ database to find the relevant details about the course.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"class about\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"class about\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_JNhd1ocBfHUMBoojltFPpn1C', 'output': '[\\n  {\\n    \"text\": \"Pandas can interpret \\\\u201cstring\\\\u201d column values as \\\\u201cdatetime\\\\u201d directly when reading the CSV file using \\\\u201cpd.read_csv\\\\u201d using the parameter \\\\u201cparse_dates\\\\u201d, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\\\npandas.read_csv \\\\u2014 pandas 2.1.4 documentation (pydata.org)\\\\nExample from week 1\\\\nimport pandas as pd\\\\ndf = pd.read_csv(\\\\n\\'yellow_tripdata_2021-01.csv\\',\\\\nnrows=100,\\\\nparse_dates=[\\'tpep_pickup_datetime\\', \\'tpep_dropoff_datetime\\'])\\\\ndf.info()\\\\nwhich will output\\\\n<class \\'pandas.core.frame.DataFrame\\'>\\\\nRangeIndex: 100 entries, 0 to 99\\\\nData columns (total 18 columns):\\\\n#   Column                 Non-Null Count  Dtype\\\\n---  ------                 --------------  -----\\\\n0   VendorID               100 non-null    int64\\\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\\\n3   passenger_count        100 non-null    int64\\\\n4   trip_distance          100 non-null    float64\\\\n5   RatecodeID             100 non-null    int64\\\\n6   store_and_fwd_flag     100 non-null    object\\\\n7   PULocationID           100 non-null    int64\\\\n8   DOLocationID           100 non-null    int64\\\\n9   payment_type           100 non-null    int64\\\\n10  fare_amount            100 non-null    float64\\\\n11  extra                  100 non-null    float64\\\\n12  mta_tax                100 non-null    float64\\\\n13  tip_amount             100 non-null    float64\\\\n14  tolls_amount           100 non-null    float64\\\\n15  improvement_surcharge  100 non-null    float64\\\\n16  total_amount           100 non-null    float64\\\\n17  congestion_surcharge   100 non-null    float64\\\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\\\nmemory usage: 14.2+ KB\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"iPython - Pandas parsing dates with \\\\u2018read_csv\\\\u2019\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\\\nmodule @0x3c947bc5\\\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Spark-shell: unable to load native-hadoop library for platform - Windows\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Open a CMD terminal in administrator mode\\\\ncd %SPARK_HOME%\\\\nStart a master node: bin\\\\\\\\spark-class org.apache.spark.deploy.master.Master\\\\nStart a worker node: bin\\\\\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\\\nNow you can access Spark UI through localhost:8080\\\\nHomework for Module 5:\\\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Spark Standalone Mode on Windows\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Error:\\\\nspark.createDataFrame(df_pandas).schema\\\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\\\nSolution:\\\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don\\\\u2019t have to take out any data from your dataset. Something like this can help:\\\\ndf = spark.read \\\\\\\\\\\\n.options(\\\\nheader = \\\\\"true\\\\\", \\\\\\\\\\\\ninferSchema = \\\\\"true\\\\\", \\\\\\\\\\\\n) \\\\\\\\\\\\n.csv(\\'path/to/your/csv/file/\\')\\\\nSolution B:\\\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\\\n# Only take rows that have no null values\\\\npandas_df= pandas_df[pandas_df.notnull().all(1)]\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"TypeError when using spark.createDataFrame function on a pandas df\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Change the working directory to the spark directory:\\\\nif you have setup up your SPARK_HOME variable, use the following;\\\\ncd %SPARK_HOME%\\\\nif not, use the following;\\\\ncd <path to spark installation>\\\\nCreating a Local Spark Cluster\\\\nTo start Spark Master:\\\\nbin\\\\\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\\\nStarting up a cluster:\\\\nbin\\\\\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"How to spark standalone cluster is run on windows OS\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems the search didn’t return specific information about what the class is about. However, I can summarize based on the course title &quot;data-engineering-zoomcamp,&quot; which typically covers topics related to data engineering concepts, tools, and best practices.</p>\n",
       "<p>Generally, this course could include areas such as:</p>\n",
       "<ol>\n",
       "<li><strong>Data Ingestion</strong>: Techniques for collecting and storing data from various sources.</li>\n",
       "<li><strong>Data Processing</strong>: Using frameworks like Apache Spark for processing large datasets.</li>\n",
       "<li><strong>Data Storage</strong>: Understanding databases (SQL and NoSQL) where processed data is stored.</li>\n",
       "<li><strong>Data Pipelines</strong>: Creating workflows for data transformation and loading.</li>\n",
       "<li><strong>Cloud and Infrastructure</strong>: Implementing tools like Docker and Terraform for managing infrastructure.</li>\n",
       "</ol>\n",
       "<p>If you're looking for more specific information related to modules or areas of focus, let me know, and I can look into it further!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d86b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
