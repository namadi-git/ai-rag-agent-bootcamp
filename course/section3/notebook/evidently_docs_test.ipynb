{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06f8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e3c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import Any, Dict, List, TypedDict\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef22d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a58f7aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelso\\Documents\\github\\ai-rag-agent-bootcamp\\course\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_typing_extra.py:329: RuntimeWarning: coroutine 'run_agent' was never awaited\n",
      "  hints[name] = try_eval_type(value, globalns, localns)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\": \"run llm as a judge in Evidently\"})\n",
      "TOOL CALL (search): search({\"query\": \"llm judge functionality in Evidently\"})\n",
      "TOOL CALL (search): search({\"query\": \"using large language model as judge in Evidently\"})\n",
      "TOOL CALL (search): search({\"query\": \"Evidently llm judge example\"})\n",
      "TOOL CALL (search): search({\"query\": \"Evidently evaluate model with llm judge\"})\n",
      "TOOL CALL (search): search({\"query\": \"implement judge function with LLM in Evidently\"})\n",
      "forcing output\n"
     ]
    }
   ],
   "source": [
    "result = await main.run_agent('how do I run llm as a judge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c47beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Running LLM as a Judge\n",
      "\n",
      "## Overview\n",
      "\n",
      "To run a large language model (LLM) as a judge in Evidently, you will create an evaluation framework that utilizes the LLM to assess the quality of generated responses against defined criteria. This process involves designing a prompt for the LLM, evaluating responses, and comparing the LLM's outputs against manual labels.\n",
      "\n",
      "### References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "## Basic Requirements\n",
      "\n",
      "1. **Basic Knowledge**: Familiarity with Python.\n",
      "2. **OpenAI API Key**: Required to utilize the LLM evaluator services.\n",
      "\n",
      "### References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "## Steps to Run LLM as a Judge\n",
      "\n",
      "### 1. Installation and Imports\n",
      "You first need to install the Evidently library:\n",
      "```python\n",
      "pip install evidently\n",
      "```\n",
      "Then, import the necessary modules:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from evidently import Dataset\n",
      "from evidently import Report\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 2. Create a Toy Dataset\n",
      "You'll need to create a dataset for evaluation which includes questions, approved responses, new responses, and manual labels:\n",
      "```python\n",
      "data = {\n",
      "    'questions': [...],\n",
      "    'target_responses': [...],\n",
      "    'new_responses': [...],\n",
      "    'labels': [...],\n",
      "}\n",
      "dataset = pd.DataFrame(data)\n",
      "```\n",
      "\n",
      "### 3. Set Up the LLM Judge\n",
      "Design the prompt template for the LLM to evaluate the new responses. For example, focusing on verbosity:\n",
      "```python\n",
      "verbosity_template = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"Your criteria description\",\n",
      "    target_category=\"concise\",\n",
      "    non_target_category=\"verbose\"\n",
      ")\n",
      "```\n",
      "\n",
      "### 4. Run Evaluation\n",
      "Add the LLM evaluation as a descriptor and run the report:\n",
      "```python\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(...)\n",
      "])\n",
      "\n",
      "report = Report([\n",
      "    TextEvals()\n",
      "])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "```\n",
      "### 5. View Results\n",
      "You can view the evaluation data in a DataFrame:\n",
      "```python\n",
      "eval_dataset.as_dataframe()\n",
      "```\n",
      "\n",
      "### 6. Upload Results to Evidently Cloud (Optional)\n",
      "If you choose, you can upload the results to the Evidently Cloud for easy exploration.\n",
      "```python\n",
      "ws.add_run(project.id, my_eval, include_data=True)\n",
      "```\n",
      "\n",
      "\n",
      "### References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "## References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.output.format_article())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15b552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SearchResultArticle(found_answer=True, title='Running LLM as a Judge', sections=[Section(heading='Overview', content=\"To run a large language model (LLM) as a judge in Evidently, you will create an evaluation framework that utilizes the LLM to assess the quality of generated responses against defined criteria. This process involves designing a prompt for the LLM, evaluating responses, and comparing the LLM's outputs against manual labels.\", references=[Reference(title='LLM as a judge', filename='examples/LLM_judge.mdx')]), Section(heading='Basic Requirements', content='1. **Basic Knowledge**: Familiarity with Python.\\n2. **OpenAI API Key**: Required to utilize the LLM evaluator services.', references=[Reference(title='LLM as a judge', filename='examples/LLM_judge.mdx')]), Section(heading='Steps to Run LLM as a Judge', content='### 1. Installation and Imports\\nYou first need to install the Evidently library:\\n```python\\npip install evidently\\n```\\nThen, import the necessary modules:\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Dataset\\nfrom evidently import Report\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n### 2. Create a Toy Dataset\\nYou\\'ll need to create a dataset for evaluation which includes questions, approved responses, new responses, and manual labels:\\n```python\\ndata = {\\n    \\'questions\\': [...],\\n    \\'target_responses\\': [...],\\n    \\'new_responses\\': [...],\\n    \\'labels\\': [...],\\n}\\ndataset = pd.DataFrame(data)\\n```\\n\\n### 3. Set Up the LLM Judge\\nDesign the prompt template for the LLM to evaluate the new responses. For example, focusing on verbosity:\\n```python\\nverbosity_template = BinaryClassificationPromptTemplate(\\n    criteria=\"Your criteria description\",\\n    target_category=\"concise\",\\n    non_target_category=\"verbose\"\\n)\\n```\\n\\n### 4. Run Evaluation\\nAdd the LLM evaluation as a descriptor and run the report:\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(...)\\n])\\n\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(eval_dataset, None)\\n```\\n### 5. View Results\\nYou can view the evaluation data in a DataFrame:\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n### 6. Upload Results to Evidently Cloud (Optional)\\nIf you choose, you can upload the results to the Evidently Cloud for easy exploration.\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n', references=[Reference(title='LLM as a judge', filename='examples/LLM_judge.mdx')])], references=[Reference(title='LLM as a judge', filename='examples/LLM_judge.mdx')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e062935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    name: str\n",
    "    args: dict\n",
    "\n",
    "\n",
    "def get_tool_calls(result) -> list[ToolCall]:\n",
    "    calls = []\n",
    "\n",
    "    for m in result.new_messages():\n",
    "        for p in m.parts:\n",
    "            kind = p.part_kind\n",
    "            if kind == 'tool-call': \n",
    "                call = ToolCall(\n",
    "                    name=p.tool_name,\n",
    "                    args=json.loads(p.args)\n",
    "                )\n",
    "                calls.append(call)\n",
    "\n",
    "    return calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90542643",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No tool calls found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tool_calls = get_tool_calls(result)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tool_calls) > \u001b[32m8\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNo tool calls found\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTOOL CALLS:\u001b[39m\u001b[33m\"\u001b[39m, tool_calls)\n",
      "\u001b[31mAssertionError\u001b[39m: No tool calls found"
     ]
    }
   ],
   "source": [
    "tool_calls = get_tool_calls(result)\n",
    "assert len(tool_calls) > 8, \"No tool calls found\"\n",
    "\n",
    "print(\"TOOL CALLS:\", tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d6b8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLS: [ToolCall(name='search', args={'query': 'run llm as a judge in Evidently'}), ToolCall(name='search', args={'query': 'llm judge functionality in Evidently'}), ToolCall(name='search', args={'query': 'using large language model as judge in Evidently'}), ToolCall(name='search', args={'query': 'Evidently llm judge example'}), ToolCall(name='search', args={'query': 'Evidently evaluate model with llm judge'}), ToolCall(name='search', args={'query': 'implement judge function with LLM in Evidently'}), ToolCall(name='final_result', args={'found_answer': True, 'title': 'Running LLM as a Judge', 'sections': [{'heading': 'Overview', 'content': \"To run a large language model (LLM) as a judge in Evidently, you will create an evaluation framework that utilizes the LLM to assess the quality of generated responses against defined criteria. This process involves designing a prompt for the LLM, evaluating responses, and comparing the LLM's outputs against manual labels.\", 'references': [{'title': 'LLM as a judge', 'filename': 'examples/LLM_judge.mdx'}]}, {'heading': 'Basic Requirements', 'content': '1. **Basic Knowledge**: Familiarity with Python.\\n2. **OpenAI API Key**: Required to utilize the LLM evaluator services.', 'references': [{'title': 'LLM as a judge', 'filename': 'examples/LLM_judge.mdx'}]}, {'heading': 'Steps to Run LLM as a Judge', 'content': '### 1. Installation and Imports\\nYou first need to install the Evidently library:\\n```python\\npip install evidently\\n```\\nThen, import the necessary modules:\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Dataset\\nfrom evidently import Report\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n### 2. Create a Toy Dataset\\nYou\\'ll need to create a dataset for evaluation which includes questions, approved responses, new responses, and manual labels:\\n```python\\ndata = {\\n    \\'questions\\': [...],\\n    \\'target_responses\\': [...],\\n    \\'new_responses\\': [...],\\n    \\'labels\\': [...],\\n}\\ndataset = pd.DataFrame(data)\\n```\\n\\n### 3. Set Up the LLM Judge\\nDesign the prompt template for the LLM to evaluate the new responses. For example, focusing on verbosity:\\n```python\\nverbosity_template = BinaryClassificationPromptTemplate(\\n    criteria=\"Your criteria description\",\\n    target_category=\"concise\",\\n    non_target_category=\"verbose\"\\n)\\n```\\n\\n### 4. Run Evaluation\\nAdd the LLM evaluation as a descriptor and run the report:\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(...)\\n])\\n\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(eval_dataset, None)\\n```\\n### 5. View Results\\nYou can view the evaluation data in a DataFrame:\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n### 6. Upload Results to Evidently Cloud (Optional)\\nIf you choose, you can upload the results to the Evidently Cloud for easy exploration.\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n', 'references': [{'title': 'LLM as a judge', 'filename': 'examples/LLM_judge.mdx'}]}], 'references': [{'title': 'LLM as a judge', 'filename': 'examples/LLM_judge.mdx'}]})]\n"
     ]
    }
   ],
   "source": [
    "tool_calls = get_tool_calls(result)\n",
    "assert len(tool_calls) > 5, \"No tool calls found\"\n",
    "\n",
    "print(\"TOOL CALLS:\", tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b7c3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m278 packages\u001b[0m \u001b[2min 4.28s\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 337ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 184ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1miniconfig\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpluggy\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytest\u001b[0m\u001b[2m==8.4.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv add pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66804167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0\n",
      "rootdir: c:\\Users\\nelso\\Documents\\github\\ai-rag-agent-bootcamp\\course\n",
      "configfile: pyproject.toml\n",
      "plugins: anyio-4.11.0, langsmith-0.4.33, logfire-4.14.2\n",
      "collected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv run pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b33b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
