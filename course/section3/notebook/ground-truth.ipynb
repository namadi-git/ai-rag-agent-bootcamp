{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd1485-e094-4189-ab7a-f5af32317d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1dd02bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f217dbed-9bda-4aac-bc66-b9b316c7ddd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docs\n",
    "\n",
    "raw_documents = docs.read_github_data()\n",
    "documents = docs.parse_data(raw_documents)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c474c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Prompt optimization',\n",
       " 'description': '[NEW] Automated prompt optimization.',\n",
       " 'content': 'More detailed documentation coming soon. \\n\\nRead the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)',\n",
       " 'filename': 'docs/library/prompt_optimization.mdx'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d10370-1423-4d7c-9613-e1b90605b3ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data definition 11\n",
      "descriptors 12\n",
      "overview 3\n",
      "metric generators 2\n",
      "output formats 1\n",
      "introduction 22\n",
      "report 4\n",
      "add tags and metadata 2\n",
      "tests 9\n",
      "alerts 1\n",
      "add dashboard panels (api) 13\n",
      "add dashboard panels (ui) 4\n",
      "overview 2\n",
      "overview 2\n",
      "work with datasets 2\n",
      "run evals via api 2\n",
      "explore view 1\n",
      "no code evals 4\n",
      "overview 2\n",
      "batch monitoring 2\n",
      "overview 3\n",
      "introduction 2\n",
      "manage projects 4\n",
      "overview 1\n",
      "overview 1\n",
      "set up tracing 10\n",
      "evidently cloud 1\n",
      "self-hosting 5\n",
      "evidently and github actions 1\n",
      "llm evaluations 2\n",
      "llm as a judge 21\n",
      "llm-as-a-jury 9\n",
      "rag evals 13\n",
      "llm regression testing 21\n",
      "tutorials and guides 12\n",
      "evidently cloud v2 1\n",
      "migration guide 7\n",
      "open-source vs. cloud 6\n",
      "telemetry 10\n",
      "why evidently? 4\n",
      "what is evidently? 1\n",
      "all descriptors 31\n",
      "all metrics 54\n",
      "overview 1\n",
      "customize data drift 17\n",
      "custom text descriptor 3\n",
      "use huggingface models 10\n",
      "configure llm judges 26\n",
      "custom metric 4\n",
      "classification metrics 8\n",
      "data stats and quality 7\n",
      "data drift 8\n",
      "ranking and recsys metrics 10\n",
      "regression metrics 9\n",
      "classification 3\n",
      "data drift 5\n",
      "data summary 3\n",
      "recommendations 3\n",
      "regression 3\n",
      "text evals 3\n",
      "llm evaluation 9\n",
      "data and ml checks 6\n",
      "tracing 5\n",
      "adversarial testing 2\n",
      "create synthetic inputs 1\n",
      "synthetic data 1\n",
      "rag evaluation dataset 1\n",
      "why synthetic data? 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_docs = []\n",
    "total_questions = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "\n",
    "    title = doc['title'].lower()\n",
    "\n",
    "    content = doc.get('content', '')\n",
    "\n",
    "    if len(content) < 1000:\n",
    "        continue\n",
    "    \n",
    "    if 'unpublished' in title:\n",
    "        continue\n",
    "\n",
    "    if 'legacy' in title:\n",
    "        continue\n",
    "\n",
    "    if 'leftovers' in title:\n",
    "        continue\n",
    "\n",
    "    if 'updates' in title:\n",
    "        continue\n",
    "\n",
    "    num_questions = len(content) // 1000\n",
    "    total_questions = total_questions + num_questions\n",
    "    print(title, num_questions)\n",
    "    selected_docs.append(doc)\n",
    "\n",
    "total_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babdc647-46ea-4a83-8759-724f8da88ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91cd98e8-bad1-4343-bb0b-796eb3bfc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea27637b-8cfe-4728-a32e-728a160d2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_format\n",
    "    )\n",
    "\n",
    "    return (response.output_parsed, response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ff88b3-e7f2-4096-9582-045603a4b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for details, edge cases, or integration options\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1–2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abce3a28-df8c-47d4-b12b-c1f8ee1c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a realistic search-engine-style query a user might type before finding the article.\n",
    "    Each question captures the likely search phrase, a short summary answer,\n",
    "    the user's assumed skill level, and their intent (conceptual or code-focused).\n",
    "    \"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query — not a full-sentence question — phrased like something typed into Google.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1–2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user making the query.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies if the user's intent is to get a theoretical explanation ('text') or an implementation example ('code').\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured collection of human-like search queries derived from a given article.\n",
    "    Includes a brief description of the article topic and a list of generated queries.\n",
    "    Difficulty distribution: 60% beginner, 30% intermediate, 10% advanced.\n",
    "    Intent distribution: 75% code-focused, 25% concept-focused.\n",
    "    \"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article or topic these search-style questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"A list of realistic search queries with short summaries, difficulty levels, and user intent.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf5d34c-ed38-4e1d-9431-08010753f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd90b4b-ea2e-498f-a09c-06e94b82ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc):\n",
    "    content = doc['content']\n",
    "    num_questions = len(content) // 1000\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    generate {num_questions} questions for this document:\n",
    "{json.dumps(doc)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    output, usage = llm_structured(\n",
    "        instructions=instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions,\n",
    "    )\n",
    "\n",
    "    return {'doc': doc, 'questions': output, 'usage': usage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34deb7e2-cfd0-43aa-9cd5-b987e41cd70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [01:32<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    results = map_progress(pool, selected_docs, process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2a3ecf-d000-4c1f-a8de-8b3f57a750b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eba1b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " 'questions': GeneratedQuestions(description='This article provides guidance on how to map input data correctly using a `Dataset` object and `DataDefinition`, including defining column types and roles in a data processing context.', questions=[Question(question='data definition mapping examples', summary_answer='The article details how to create a `DataDefinition` to map column types and roles for data evaluation processes in Evidently.', difficulty='beginner', intent='code'), Question(question='create Dataset object Evidently', summary_answer='To create a `Dataset` object, you can use the `Dataset.from_pandas` method along with a configured `DataDefinition` to properly map your data columns.', difficulty='beginner', intent='code'), Question(question='column types in DataDefinition', summary_answer='The article explains the different column types like numerical, categorical, and text that can be mapped using `DataDefinition`, crucial for correct data evaluation.', difficulty='intermediate', intent='text'), Question(question='define regression mappings', summary_answer='For regression checks, you must designate your target and prediction columns using a `DataDefinition`, as illustrated in the provided code examples.', difficulty='intermediate', intent='code'), Question(question='using pandas DataFrame with Evidently', summary_answer=\"You can pass a `pandas.DataFrame` directly to evidently's report functions, but it's recommended to create a `Dataset` object for better control and clarity.\", difficulty='intermediate', intent='text'), Question(question='manual vs automatic column mapping Evidently', summary_answer='The article discusses the pros and cons of manual mapping versus automatic mapping of columns in `DataDefinition` when preparing datasets for evaluation.', difficulty='advanced', intent='text'), Question(question='map target and prediction columns', summary_answer='The framework allows you to specify your target and prediction columns for regression models using the `DataDefinition`, ensuring your evaluations run correctly.', difficulty='intermediate', intent='code'), Question(question='examples of text columns mapping', summary_answer='You can define text columns in `DataDefinition` for LLM evaluations, ensuring they are processed correctly in the evaluation pipeline.', difficulty='beginner', intent='code'), Question(question='Evidently column role assignments', summary_answer='The article explains how to specify roles such as `id` and `timestamp` in your `DataDefinition`, which help the evaluation framework function correctly.', difficulty='intermediate', intent='text'), Question(question='input data flexibility Evidently', summary_answer='The article notes the flexibility in input data structure when using Evidently, allowing a mix of data types in the `Dataset` for evaluations.', difficulty='beginner', intent='text'), Question(question='multiple datasets for drift detection', summary_answer='If evaluating drift, it’s necessary to create two datasets, ensuring both adhere to the same data definition for accuracy in comparisons.', difficulty='advanced', intent='code')]),\n",
       " 'usage': ResponseUsage(input_tokens=3439, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=570, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=4009)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23539e-20ff-4c45-9ad1-744b56407354",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'toyaikit.pricing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtoyaikit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpricing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PricingConfig\n\u001b[32m      3\u001b[39m pricing = PricingConfig()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'toyaikit.pricing'"
     ]
    }
   ],
   "source": [
    "# from toyaikit.pricing import PricingConfig\n",
    "# # \n",
    "# pricing = PricingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ceb8cdf-d317-42e7-bc5f-0206a51df18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = 0\n",
    "output_tokens = 0\n",
    "\n",
    "for r in results:\n",
    "    usage = r['usage']\n",
    "    input_tokens = input_tokens + usage.input_tokens\n",
    "    output_tokens = output_tokens + usage.output_tokens\n",
    "    \n",
    "# pricing.calculate_cost('gpt-4o-mini', input_tokens, output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aac5fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173694, 22996)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dac0a27-f8c8-4138-9f95-6733fdbd92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_questions = []\n",
    "\n",
    "for r in results:\n",
    "    doc = r['doc']\n",
    "    questions = r['questions']\n",
    "\n",
    "    for q in questions.questions:\n",
    "        final_question = q.model_dump()\n",
    "        final_question['filename'] = doc['filename']\n",
    "        final_questions.append(final_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "473ce08f-46a2-4291-9410-0dfeb1569951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3441d4fd-fe64-42ad-a500-86d3ee488c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(final_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5e93a01-dcf4-4ee4-aa07-ca3fbd73316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv('ground_truth_evidently.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "535888a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>summary_answer</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>intent</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data definition mapping examples</td>\n",
       "      <td>The article details how to create a `DataDefin...</td>\n",
       "      <td>beginner</td>\n",
       "      <td>code</td>\n",
       "      <td>docs/library/data_definition.mdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create Dataset object Evidently</td>\n",
       "      <td>To create a `Dataset` object, you can use the ...</td>\n",
       "      <td>beginner</td>\n",
       "      <td>code</td>\n",
       "      <td>docs/library/data_definition.mdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>column types in DataDefinition</td>\n",
       "      <td>The article explains the different column type...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>text</td>\n",
       "      <td>docs/library/data_definition.mdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>define regression mappings</td>\n",
       "      <td>For regression checks, you must designate your...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>code</td>\n",
       "      <td>docs/library/data_definition.mdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>using pandas DataFrame with Evidently</td>\n",
       "      <td>You can pass a `pandas.DataFrame` directly to ...</td>\n",
       "      <td>intermediate</td>\n",
       "      <td>text</td>\n",
       "      <td>docs/library/data_definition.mdx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                question  \\\n",
       "0       data definition mapping examples   \n",
       "1        create Dataset object Evidently   \n",
       "2         column types in DataDefinition   \n",
       "3             define regression mappings   \n",
       "4  using pandas DataFrame with Evidently   \n",
       "\n",
       "                                      summary_answer    difficulty intent  \\\n",
       "0  The article details how to create a `DataDefin...      beginner   code   \n",
       "1  To create a `Dataset` object, you can use the ...      beginner   code   \n",
       "2  The article explains the different column type...  intermediate   text   \n",
       "3  For regression checks, you must designate your...  intermediate   code   \n",
       "4  You can pass a `pandas.DataFrame` directly to ...  intermediate   text   \n",
       "\n",
       "                           filename  \n",
       "0  docs/library/data_definition.mdx  \n",
       "1  docs/library/data_definition.mdx  \n",
       "2  docs/library/data_definition.mdx  \n",
       "3  docs/library/data_definition.mdx  \n",
       "4  docs/library/data_definition.mdx  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('ground_truth_evidently.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca7828-8282-4be7-91fb-26a3ac98b6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
