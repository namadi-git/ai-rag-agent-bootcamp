question,summary_answer,difficulty,intent,filename
data definition mapping examples,The article details how to create a `DataDefinition` to map column types and roles for data evaluation processes in Evidently.,beginner,code,docs/library/data_definition.mdx
create Dataset object Evidently,"To create a `Dataset` object, you can use the `Dataset.from_pandas` method along with a configured `DataDefinition` to properly map your data columns.",beginner,code,docs/library/data_definition.mdx
column types in DataDefinition,"The article explains the different column types like numerical, categorical, and text that can be mapped using `DataDefinition`, crucial for correct data evaluation.",intermediate,text,docs/library/data_definition.mdx
define regression mappings,"For regression checks, you must designate your target and prediction columns using a `DataDefinition`, as illustrated in the provided code examples.",intermediate,code,docs/library/data_definition.mdx
using pandas DataFrame with Evidently,"You can pass a `pandas.DataFrame` directly to evidently's report functions, but it's recommended to create a `Dataset` object for better control and clarity.",intermediate,text,docs/library/data_definition.mdx
manual vs automatic column mapping Evidently,The article discusses the pros and cons of manual mapping versus automatic mapping of columns in `DataDefinition` when preparing datasets for evaluation.,advanced,text,docs/library/data_definition.mdx
map target and prediction columns,"The framework allows you to specify your target and prediction columns for regression models using the `DataDefinition`, ensuring your evaluations run correctly.",intermediate,code,docs/library/data_definition.mdx
examples of text columns mapping,"You can define text columns in `DataDefinition` for LLM evaluations, ensuring they are processed correctly in the evaluation pipeline.",beginner,code,docs/library/data_definition.mdx
Evidently column role assignments,"The article explains how to specify roles such as `id` and `timestamp` in your `DataDefinition`, which help the evaluation framework function correctly.",intermediate,text,docs/library/data_definition.mdx
input data flexibility Evidently,"The article notes the flexibility in input data structure when using Evidently, allowing a mix of data types in the `Dataset` for evaluations.",beginner,text,docs/library/data_definition.mdx
multiple datasets for drift detection,"If evaluating drift, it’s necessary to create two datasets, ensuring both adhere to the same data definition for accuracy in comparisons.",advanced,code,docs/library/data_definition.mdx
create descriptors for text evaluation,Descriptors are universal interfaces for evaluating text data. You can create them using built-in options or custom prompts.,beginner,code,docs/library/descriptors.mdx
how to use evidently library for text evaluation,"The Evidently library allows you to evaluate text data by defining descriptors, utilizing various built-in metrics and customizing them for specific needs.",beginner,text,docs/library/descriptors.mdx
generate sample data for text evaluation,You can create toy data using pandas to simulate questions and answers for testing text evaluation descriptors.,beginner,code,docs/library/descriptors.mdx
combine multiple descriptors in evident,"Evidently allows you to combine multiple descriptors to evaluate and score datasets effectively, facilitating comprehensive insights into text data.",intermediate,code,docs/library/descriptors.mdx
export results from text evaluation,You can export evaluation results to various formats like HTML or JSON directly from the Evidently library report generation process.,intermediate,code,docs/library/descriptors.mdx
using LLM judges for evaluation,"You can integrate external LLMs as judges for scoring text data, providing an advanced evaluation mechanism when added as descriptors.",advanced,code,docs/library/descriptors.mdx
how to add tests to descriptors,"You can set pass/fail conditions in your descriptors by adding tests that check various attributes, enhancing your evaluation strategy.",intermediate,code,docs/library/descriptors.mdx
understanding descriptor parameters,"Each descriptor may have specific parameters essential for its functionality, which can be configured according to your evaluation needs.",intermediate,text,docs/library/descriptors.mdx
report generation in evidently library,"Evidently allows you to generate detailed reports summarizing evaluation results and descriptors, making it easier to analyze insights.",beginner,text,docs/library/descriptors.mdx
multi-column descriptors for semantic similarity,"Use multi-column descriptors to evaluate semantic similarity, allowing you to compare two text columns for nuanced assessments.",advanced,code,docs/library/descriptors.mdx
custom descriptors implementation,You can implement your custom descriptors in Python to tailor your text evaluations beyond the standard options provided by the library.,advanced,code,docs/library/descriptors.mdx
integration of metadata checks in evaluation,"You can apply checks to existing columns, like metadata, using ColumnTest, which enhances the robustness of your text evaluation framework.",intermediate,code,docs/library/descriptors.mdx
Evidently eval workflow steps,"The article outlines the eval workflow using Evidently, including key steps like preparing input data, creating a Dataset object, and running reports.",beginner,text,docs/library/evaluations_overview.mdx
Create Dataset object with Evidently,"To create a Dataset object, use the `Dataset.from_pandas()` method with a `DataDefinition()` to specify column roles and types, as described in the article.",intermediate,code,docs/library/evaluations_overview.mdx
Add descriptors in Evidently eval,"For text evaluations, you can add descriptors using the `add_descriptors()` method to compute row-level metrics, enhancing the evaluation process.",advanced,code,docs/library/evaluations_overview.mdx
metric generator helper functions overview,The article provides a comprehensive guide on using metric generator helper functions to simplify the generation of multiple column-level tests or metrics in datasets.,beginner,text,docs/library/metric_generator.mdx
applying ValueDrift to all columns code example,"It demonstrates how to apply the ValueDrift metric to all columns in a dataset using the ColumnMetricGenerator within the Evidently library, complete with example code.",intermediate,code,docs/library/metric_generator.mdx
export evaluation results formats,"The article explains how to export evaluation results in various formats such as HTML, JSON, and Python dictionaries, providing code examples for each format.",beginner,code,docs/library/output_formats.mdx
Evidently Python library overview,"The Evidently Python library is an open-source tool for evaluating and monitoring AI systems, providing various built-in metrics and customizable evaluations.",beginner,text,docs/library/overview.mdx
how to use Evidently for AI evaluation,"Evidently allows users to run AI evaluations on datasets, providing scores and visual reports to assess model performance and data quality.",beginner,code,docs/library/overview.mdx
examples of AI metrics in Evidently,"The library includes over 100 built-in metrics for AI evaluations, such as accuracy, precision, recall, and data quality checks like missing values and duplicates.",beginner,text,docs/library/overview.mdx
Evidently library features,"The library features AI/ML evaluations, synthetic data generation, prompt optimization tools, and a UI for tracking evaluation results.",beginner,text,docs/library/overview.mdx
how to generate synthetic data with Evidently,"Evidently provides a configuration for generating structured synthetic data specifically for LLM use cases, making it useful for testing AI applications.",intermediate,code,docs/library/overview.mdx
integrating Evidently into data workflows,"Evidently's export features allow integration into existing pipelines by exporting metrics as JSON, DataFrames, or visual reports.",intermediate,code,docs/library/overview.mdx
setting up test conditions in Evidently,"Users can create tests in Evidently to validate metrics against expected conditions, enhancing the evaluation process for AI systems.",intermediate,code,docs/library/overview.mdx
Evidently metrics presets,Metrics presets in Evidently allow for pre-configured evaluations to summarize different aspects of data and model performance in one step.,intermediate,text,docs/library/overview.mdx
overview of metrics and descriptors in Evidently,"Metrics evaluate overall data performance, while descriptors provide row-level assessments, useful for detailed evaluation of text and LLM outputs.",intermediate,text,docs/library/overview.mdx
visual reporting with Evidently,"Evidently supports visual reporting for evaluations, enabling users to generate and present metrics and outcomes in an easy-to-understand format.",beginner,code,docs/library/overview.mdx
different types of evaluations in Evidently,"Evidently supports various evaluations, including classification, regression, text quality, and data drift assessments, tailored to specific use cases.",beginner,text,docs/library/overview.mdx
using LLMs in Evidently,"The library offers specialized tools for evaluating LLM outputs, including semantic similarity checks and relevance assessments for AI-generated content.",intermediate,text,docs/library/overview.mdx
Evidently cloud vs self-hosting,"Evidently can either be self-hosted or used via the Evidently Cloud, with different capabilities, including additional features in the Cloud version.",beginner,text,docs/library/overview.mdx
Python code examples for Evidently,Evidently provides code examples and templates to help users get started with AI evaluations efficiently and effectively in Python.,beginner,code,docs/library/overview.mdx
best practices for using Evidently,"Best practices for using Evidently involve preparing datasets correctly, utilizing metrics and descriptors, and generating comprehensive reports for evaluations.",intermediate,text,docs/library/overview.mdx
Evidently evaluation results format,"Evaluation results from Evidently can be exported as JSON, DataFrames, or visual reports to facilitate integration and monitoring.",beginner,text,docs/library/overview.mdx
monitoring AI system performance with Evidently,"Users can use Evidently's features to monitor AI system performance over time, checking for data drift and model accuracy adjustments.",intermediate,text,docs/library/overview.mdx
descriptor tests in Evidently,Descriptor tests in Evidently allow users to add pass/fail conditions to row-level outputs to ensure quality and relevance in responses.,advanced,code,docs/library/overview.mdx
data drift detection using Evidently,"Evidently includes features for detecting data drift, comparing current datasets against reference datasets to identify shifts in data distribution.",intermediate,text,docs/library/overview.mdx
LLM logs for Evidently,"Evidently can process LLM logs, enabling users to evaluate outputs, input context, and ground truth answers for quality assessments.",beginner,code,docs/library/overview.mdx
conditional testing in Evidently,"Evidently allows for the setup of conditional tests on metrics, enabling detailed checks against specific expectations during evaluations.",advanced,code,docs/library/overview.mdx
evidently report generation basics,"The article outlines the fundamental steps to generate reports using the Evidently library, including prerequisites and basic examples for single and multiple datasets.",beginner,text,docs/library/report.mdx
how to run a report with two datasets,"It explains how to run a report using two datasets, adding details on how to specify the current and reference datasets for comparisons like data drift.",intermediate,code,docs/library/report.mdx
evidently report presets examples,"The article provides examples of using preset metrics to generate reports quickly, illustrating how to run a report using a DataSummaryPreset or a DataDriftPreset.",beginner,code,docs/library/report.mdx
custom metrics in evidently reports,"It describes how to create custom reports by listing specific metrics, including examples of using column-level metrics and setting parameters for precision calculations.",advanced,code,docs/library/report.mdx
adding tags and metadata to reports in Evidently,"The article details how to add tags and metadata to Reports in Evidently, which helps in organizing and filtering the evaluations by specific model versions, test scenarios, and statuses.",beginner,text,docs/library/tags_metadata.mdx
custom tags and metadata example in Python,"It provides code examples demonstrating how to include custom tags and metadata as Python dictionaries or lists when creating and running Reports, enabling contextual filtering and visualization.",intermediate,code,docs/library/tags_metadata.mdx
how to validate data conditions,"The article outlines how to validate data conditions by using Tests to get Pass/Fail results on datasets, focusing on preset conditions or customizable checks.",beginner,text,docs/library/tests.mdx
test presets in evidently,It covers how to use Test Presets to automatically generate conditions for evaluating your data or AI systems within Evidently reports.,beginner,code,docs/library/tests.mdx
setting custom test conditions,"You can define specific pass/fail conditions for each Test in the Report using parameters like gt, lt, eq, etc., as explained in the article.",intermediate,code,docs/library/tests.mdx
usage of metrics in data tests,The article details how to import and use various metrics along with Tests to validate the dataset effectively.,beginner,code,docs/library/tests.mdx
examples of conditional checks,"It provides examples of setting up conditional checks, such as checking for missing values or ensuring minimum age thresholds in datasets.",intermediate,code,docs/library/tests.mdx
report generation with tests,"You can generate reports that include Tests by setting `include_tests=True`, allowing for additional validations on the dataset.",beginner,code,docs/library/tests.mdx
custom vs default test configurations,The article explains the difference and use cases for custom tests versus default tests when running your data validations.,intermediate,text,docs/library/tests.mdx
test result criticality settings,"It describes how to customize the criticality of test results, allowing for different levels of alerts based on test outcomes.",advanced,code,docs/library/tests.mdx
reference dataset testing,"You can perform tests against a reference dataset to set conditions relative to the reference values, improving validation accuracy.",advanced,code,docs/library/tests.mdx
set up alerts in Evidently,"The article provides a step-by-step guide on configuring alerts in Evidently, including how to choose notification channels and set alert conditions for failed tests.",beginner,code,docs/platform/alerts.mdx
add panels python api,"The article explains how to add custom Panels to a Dashboard using the Python API, detailing various types such as text panels and counters.",beginner,code,docs/platform/dashboard_add_panels.mdx
delete dashboard tab,You can delete a Tab from your Dashboard using the Python API with the command provided in the article.,beginner,code,docs/platform/dashboard_add_panels.mdx
create dashboard tab with panel,"Learn how to create a new Tab while adding a Panel, with specific instructions in the article.",beginner,code,docs/platform/dashboard_add_panels.mdx
dashboard panel types explained,"The article describes different types of Panels that can be added to a Dashboard, including text and counter panels and their specific implementations.",intermediate,text,docs/platform/dashboard_add_panels.mdx
python api dashboard clear all panels,"To remove all Panels and Tabs from the Dashboard, the article provides the command to execute, ensuring reports remain intact.",intermediate,code,docs/platform/dashboard_add_panels.mdx
multiple panels addition,You can add multiple Panels at once and the article provides code examples to illustrate how to implement this effectively.,intermediate,code,docs/platform/dashboard_add_panels.mdx
panel metric options,"The article includes a summary of options available for PanelMetric, which are essential for defining what values to display in Panels.",intermediate,text,docs/platform/dashboard_add_panels.mdx
python dashboard counter example,Examples of adding counter panels to a Dashboard using the Python API are provided in the article for quick reference.,beginner,code,docs/platform/dashboard_add_panels.mdx
aggregate metrics in dashboard,"You can aggregate various metrics like sum, average, and last for Panels; this is detailed in the article with example code.",intermediate,code,docs/platform/dashboard_add_panels.mdx
what is dashboard panel customization,"The article explains customization options for Dashboard panels, allowing users to tailor the presentation of metrics based on needs.",beginner,text,docs/platform/dashboard_add_panels.mdx
use of panel metric in python,Instructions on how to define and use PanelMetric within your Panel in the Dashboard are thoroughly outlined in the article.,advanced,code,docs/platform/dashboard_add_panels.mdx
configuring panel values python,"The article provides insights into how to configure and specify values for Panels using Python, including metric labels and legends.",advanced,code,docs/platform/dashboard_add_panels.mdx
dashboard layout options,Details on layout settings like panel size (full or half) and how to organize them in Tabs are discussed in the article.,beginner,text,docs/platform/dashboard_add_panels.mdx
create custom dashboard panels,"The article provides a step-by-step guide on creating custom dashboard panels, including how to add and configure them within the dashboard interface.",beginner,code,docs/platform/dashboard_add_panels_ui.mdx
how to add tabs in dashboard,"Instructions for adding tabs to the dashboard for better organization are included, allowing users to create custom or use pre-built tabs for their panels.",beginner,code,docs/platform/dashboard_add_panels_ui.mdx
panel configuration settings,"The article outlines the necessary configuration settings for dashboard panels, such as selecting metrics, filtering options, and panel types to visualize data.",intermediate,code,docs/platform/dashboard_add_panels_ui.mdx
dashboard panel editing and deleting,"It details how to edit or delete existing panels, ensuring users can manage their dashboard effectively after initial setup.",intermediate,code,docs/platform/dashboard_add_panels_ui.mdx
Evidently Dashboard features,"The article outlines key features of the Evidently Dashboard, which include tracking AI application performance, organizing Panels in Tabs, and visualizing data through custom Panels.",beginner,text,docs/platform/dashboard_overview.mdx
Adding Panels to Evidently Dashboard,"To add Panels to the Evidently Dashboard, you can either define them using the Python API or add them directly through the user interface, selecting from various metrics and custom parameters.",intermediate,code,docs/platform/dashboard_overview.mdx
creating datasets in Evidently,"The article details various methods to create datasets in Evidently, including uploading CSV files, generating synthetic data, and creating datasets from traces or reports.",beginner,code,docs/platform/datasets_overview.mdx
what is a dataset in data analysis,"Datasets are collections of data from applications used for analysis and automated checks, and can be uploaded, generated, or created from traces according to the article.",beginner,text,docs/platform/datasets_overview.mdx
upload dataset python example,"The article explains how to use the `add_dataset` method in Python to upload a dataset to a specified project, providing an example of preparing the dataset as an Evidently Dataset.",beginner,code,docs/platform/datasets_workflow.mdx
how to include dataset in reports Evidently,"It outlines the process of including datasets when uploading reports to the platform, highlighting the use of the `include_data` parameter to upload both the report and the dataset together.",intermediate,code,docs/platform/datasets_workflow.mdx
evidently eval API example code,The article provides a code example demonstrating how to use the Evidently API to run an evaluation and log results in a workspace.,beginner,code,docs/platform/evals_api.mdx
how to upload data in evidently evals,"Uploading data in Evidently evals involves choosing to include the raw dataset or just the evaluation metrics during the upload process, as explained in the article.",intermediate,text,docs/platform/evals_api.mdx
how to explore evaluation reports,"The article explains that to explore evaluation reports, you should navigate to the Reports section of your Project and click on 'Explore' next to an individual Report to view metrics and datasets.",beginner,text,docs/platform/evals_explore.mdx
no code data evaluation steps,"The article outlines a step-by-step process for evaluating data without coding, including preparing datasets and configuring evaluations using descriptors.",beginner,text,docs/platform/evals_no_code.mdx
how to upload datasets no code,"You can upload datasets by creating a new dataset from a CSV or selecting an existing one, as detailed in the article.",beginner,code,docs/platform/evals_no_code.mdx
examples of descriptors in no code evals,"The article provides examples of different descriptors used in evaluations, such as semantic similarity and custom LLM evaluators, including how to configure them.",intermediate,code,docs/platform/evals_no_code.mdx
LLM provider API key in evaluations,"Including the LLM provider API key is optional for evaluations, allowing users to leverage external LLMs; otherwise, users can proceed with built-in methods.",advanced,text,docs/platform/evals_no_code.mdx
running evaluations in Evidently,"The article outlines how to run various AI evaluations using the Evidently platform, including options for code-based as well as no-code evaluations.",beginner,text,docs/platform/evals_overview.mdx
examples of AI product evaluations,"It explains different stages of AI product evaluations, such as ad hoc analysis, experiments, and monitoring, with guidance on implementing them on the Evidently platform.",intermediate,code,docs/platform/evals_overview.mdx
evidently batch monitoring overview,"The article provides an overview of batch monitoring with the Evidently Python library, explaining the core evaluation API and showing how to run evaluation jobs effectively.",beginner,text,docs/platform/monitoring_local_batch.mdx
python batch monitoring example,"The article includes a simple Python example for batch monitoring, demonstrating how to get dataset stats and upload them to the workspace using the Evidently library.",intermediate,code,docs/platform/monitoring_local_batch.mdx
evidently AI quality monitoring setup,"The article outlines how to set up AI quality monitoring using Evidently, highlighting batch monitoring jobs and tracing evaluations to ensure system behavior is continuously evaluated.",beginner,code,docs/platform/monitoring_overview.mdx
batch monitoring jobs explanation,"It discusses the concept of batch monitoring jobs, which are ideal for ML pipelines and involve creating an evaluation pipeline to regularly compute metrics and visualize results.",intermediate,text,docs/platform/monitoring_overview.mdx
schedule evaluations in Evidently,"The article explains how to set up scheduled evaluations with the Tracely library, allowing for automated assessments of application inputs and outputs on the Evidently platform.",advanced,code,docs/platform/monitoring_overview.mdx
Evidently Platform key features summary,"The article outlines the key features of the Evidently Platform, including evaluation management, dataset organization, synthetic data generation, regression testing, live monitoring, and tracing capabilities.",beginner,text,docs/platform/overview.mdx
Evidently Python library usage,"It explains that the Evidently Python library allows users to run local evaluations and interact programmatically with the Evidently Cloud API, facilitating dataset uploads and experimental evaluations.",intermediate,code,docs/platform/overview.mdx
create project in Evidently Cloud,"To create a project in Evidently Cloud, use `ws.create_project(""Project Name"", org_id=""YOUR_ORG_ID"")` for a cloud setup or simply `ws.create_project(""Project Name"")` for self-hosted environments.",beginner,code,docs/platform/projects_manage.mdx
connect to an existing project Python,"To connect to an existing project in Python, use the `get_project` method with the specific Project ID: `ws.get_project(""PROJECT_ID"")`.",intermediate,code,docs/platform/projects_manage.mdx
delete project in Evidently,"Deleting a project in Evidently can be done by either calling `ws.delete_project(""PROJECT ID"")` in Python or using the UI to hover over the project and click ""delete"".",beginner,code,docs/platform/projects_manage.mdx
Project parameters in Evidently,"Each project in Evidently includes parameters such as name, ID, description, and dashboard configuration, which can be specified during project creation.",advanced,text,docs/platform/projects_manage.mdx
what are projects in evidently,"Projects in Evidently help organize data and evaluations for specific use cases, storing datasets, reports, and providing unique dashboards and alerting rules.",beginner,text,docs/platform/projects_overview.mdx
LLM tracing features Evidently,"The article explains that LLM tracing in Evidently allows you to collect detailed operational data of your AI application, helping in evaluation and analysis through various views like trace, dataset, and dialogue.",beginner,text,docs/platform/tracing_overview.mdx
install tracely package,The article instructs to install the `tracely` package using pip for tracing integration.,beginner,code,docs/platform/tracing_setup.mdx
how to initialize tracing,"It explains how to initialize tracing using the `init_tracing` function with necessary parameters like address, API key, and project ID.",beginner,code,docs/platform/tracing_setup.mdx
tracely init_tracing function arguments,"The article lists parameters for the `init_tracing()` function, including descriptions of each and their corresponding environment variables.",intermediate,text,docs/platform/tracing_setup.mdx
using trace_event decorator,"You can use the `trace_event` decorator for functions to start collecting traces, as illustrated with examples in the article.",beginner,code,docs/platform/tracing_setup.mdx
get_info function for export id,It shows how to use the `get_info()` function to retrieve the export ID of the tracing dataset.,intermediate,code,docs/platform/tracing_setup.mdx
nesting trace_event decorators,It discusses how to nest `trace_event` decorators for tracing multi-step workflows and provides an example implementation.,intermediate,code,docs/platform/tracing_setup.mdx
create_trace_event context manager usage,"The context manager `create_trace_event` allows for inline tracing of specific code segments without using decorators, as detailed in the article.",intermediate,code,docs/platform/tracing_setup.mdx
event attributes in tracing,"You can add custom attributes to active spans using the `get_current_span()` method, enhancing trace detail.",advanced,code,docs/platform/tracing_setup.mdx
how to handle multiple spans,"The article describes managing multiple spans through functions or threads, emphasizing session management with `session_id`.",intermediate,text,docs/platform/tracing_setup.mdx
linking events with bind_to_trace,"It details how to link events into a single trace using `tracely.bind_to_trace`, ensuring all events share the same trace ID.",advanced,code,docs/platform/tracing_setup.mdx
Evidently Cloud account setup steps,"The article outlines how to create an account on Evidently Cloud, set up an organization, and connect using Python with an API token.",beginner,code,docs/setup/cloud.mdx
self-hosting Evidently UI guide,"The article provides a detailed guide on how to self-host the Evidently UI service, including creating workspaces and launching the service.",beginner,text,docs/setup/self-hosting.mdx
create a workspace for Evidently,"It explains how to create a local or remote workspace in Evidently to store evaluation results, with code snippets for implementation.",beginner,code,docs/setup/self-hosting.mdx
Evidently UI service setup,The article walks through the necessary steps to set up the Evidently UI service and how to launch it after creating a workspace.,intermediate,code,docs/setup/self-hosting.mdx
connecting to remote snapshot storage in Evidently,"It details how to connect the Evidently service to remote data stores, including usage of the Fsspec library for accessing snapshots.",advanced,code,docs/setup/self-hosting.mdx
how to delete a workspace in Evidently,The article provides instructions on safely deleting a workspace and the implications of losing data stored within it.,intermediate,text,docs/setup/self-hosting.mdx
testing LLM outputs with GitHub Actions,The article outlines how to use Evidently in conjunction with GitHub Actions to facilitate automated testing of LLM outputs during code integration workflows.,beginner,code,examples/GitHub_actions.mdx
LLM evaluation methods overview,"The article presents an overview of different LLM evaluation methods, detailing how they function and providing resources for further learning.",beginner,text,examples/LLM_evals.mdx
code examples for LLM evaluations,"The article includes code examples and notebooks demonstrating implementation for various LLM evaluation methods, helpful for those looking to apply these techniques directly.",intermediate,code,examples/LLM_evals.mdx
create LLM evaluator tutorial,"The article outlines a tutorial for creating an LLM evaluator, detailing steps for dataset creation and evaluation methods.",beginner,code,examples/LLM_judge.mdx
reference-based evaluation LLM,"It describes a reference-based evaluation method for LLMs, comparing responses against approved answers.",intermediate,text,examples/LLM_judge.mdx
Python package for LLM evaluation,The tutorial recommends using the Evidently Python package for evaluating LLMs effectively and provides installation steps.,beginner,code,examples/LLM_judge.mdx
how to test LLM responses,The article explains how to use an LLM as a judge for various responses and the criteria for correctness.,intermediate,text,examples/LLM_judge.mdx
evaluate LLM accuracy,It discusses evaluating LLM accuracy against a manually labeled dataset to assess performance.,intermediate,text,examples/LLM_judge.mdx
install Evidently package,"Instructions for installing the Evidently package are provided, allowing users to get started with LLM evaluation.",beginner,code,examples/LLM_judge.mdx
create Q&A dataset for LLM,The article shows how to create a toy Q&A dataset that can be used for evaluating the LLM judge.,beginner,code,examples/LLM_judge.mdx
LLM judge open-ended evaluation,It covers the open-ended evaluation method for assessing LLM outputs when a reference is not available.,intermediate,text,examples/LLM_judge.mdx
generate synthetic data for LLM,The article notes that users can generate synthetic data using the Evidently Platform for LLM evaluation.,intermediate,code,examples/LLM_judge.mdx
adding descriptors to LLM evaluation,The tutorial provides guidance on adding descriptors for evaluating new responses with the LLM judge.,intermediate,code,examples/LLM_judge.mdx
how to improve LLM judge prompts,It discusses iterating on evaluation prompts to enhance the accuracy of the LLM judge's assessments.,advanced,text,examples/LLM_judge.mdx
report generation for LLM evaluation,Instructions for generating reports to summarize LLM evaluation results are detailed in the article.,intermediate,code,examples/LLM_judge.mdx
correctness evaluator in LLM,The article describes setting up a correctness evaluator that checks responses against reference answers.,intermediate,code,examples/LLM_judge.mdx
LLM judge final results analysis,It emphasizes the importance of analyzing the final results of the LLM judge using various metrics like accuracy and recall.,advanced,text,examples/LLM_judge.mdx
Code example for LLM evaluation,Code snippets are provided throughout the article to illustrate how to implement LLM evaluations in Python.,beginner,code,examples/LLM_judge.mdx
confusion matrix for LLM outputs,The article mentions generating a confusion matrix to visualize LLM evaluation results and errors.,advanced,code,examples/LLM_judge.mdx
Evidently Cloud integration for LLM,It explains how to integrate the LLM judge with Evidently Cloud for better results tracking and sharing.,intermediate,code,examples/LLM_judge.mdx
criteria for LLM judging,"The article outlines criteria for evaluating responses, which includes correctness and verbosity checks.",intermediate,text,examples/LLM_judge.mdx
LLM judge prompt example,Examples of how to write effective prompts for the LLM judge are provided to improve evaluation outcomes.,advanced,code,examples/LLM_judge.mdx
how to contact Evidently support,The article briefly touches on how users can reach out for support with Evidently-related queries.,beginner,text,examples/LLM_judge.mdx
evaluate outputs using multiple LLMs,"The article discusses how to use multiple LLMs to aggregate evaluation results, determining a pass condition based on majority approval.",beginner,text,examples/LLM_jury.mdx
setting up LLM judges for evaluation,"It explains how to set up LLM judges, including passing API keys for different models and defining evaluation criteria.",intermediate,code,examples/LLM_jury.mdx
how to install evidently for LLM evaluation,"The article outlines installation steps for the Evidently package required for conducting LLM evaluations, including specific components to import.",beginner,code,examples/LLM_jury.mdx
example of generating emails for LLM evaluation,It provides sample datasets simulating user inputs and generated emails that can be judged for appropriateness by LLMs.,beginner,code,examples/LLM_jury.mdx
understanding disagreement among LLM judges,"The article discusses how to explicitly flag disagreements among LLMs during evaluation, indicating when they do not unanimously approve or reject outputs.",intermediate,text,examples/LLM_jury.mdx
create evaluation reports from LLM outputs,"It explains how to run evaluations and generate reports summarizing the LLM assessments, including metrics and individual judgments on outputs.",intermediate,code,examples/LLM_jury.mdx
criteria for judging email appropriateness,The article defines the criteria that LLM judges evaluate when determining the appropriateness of generated email content.,beginner,text,examples/LLM_jury.mdx
using CloudWorkspace in LLM evaluation,It describes how to set up a CloudWorkspace for managing projects and storing evaluation results within Evidently.,intermediate,code,examples/LLM_jury.mdx
advanced configuration for LLM evaluations,The article includes advanced details on configuring custom descriptors to analyze the success rates of LLM judgments.,advanced,code,examples/LLM_jury.mdx
evaluating retrieval quality in RAG systems,"The article outlines methods to assess the quality of retrieved contexts in RAG systems, focusing on metrics for both single and multiple context retrieval.",beginner,text,examples/LLM_rag_evals.mdx
RAG generation quality metrics,"It explains how to evaluate the generation quality of RAG outputs, including comparisons with ground truth and using various metrics like correctness and faithfulness.",beginner,text,examples/LLM_rag_evals.mdx
pandas dataframe for RAG evaluation,The tutorial demonstrates how to create and view evaluation results using a pandas dataframe to summarize performance metrics for RAG systems.,beginner,code,examples/LLM_rag_evals.mdx
installing Evidently library,Instructions for installing the Evidently library used for RAG evaluations are provided in the article along with Python import examples.,beginner,code,examples/LLM_rag_evals.mdx
context relevance scoring in RAG,"RAG evaluation methods include scoring the relevance of contexts retrieved, highlighting how to implement these metrics using Evidently tools.",intermediate,code,examples/LLM_rag_evals.mdx
generate synthetic dataset for evaluation,"The article explains how to create synthetic datasets to simulate the evaluation of RAG systems, offering sample code for generating data.",intermediate,code,examples/LLM_rag_evals.mdx
reporting metrics for RAG evaluations,"It discusses how to compile evaluation metrics into structured reports for RAG systems, allowing users to visualize performance results more effectively.",intermediate,text,examples/LLM_rag_evals.mdx
mean relevance scoring in RAG,The tutorial highlights how to compute mean relevance scores from multiple contexts to assess retrieval performance in RAG systems.,intermediate,code,examples/LLM_rag_evals.mdx
adding tests to RAG evaluations,The article provides guidance on setting up tests within Evidently to ensure RAG models meet specified performance criteria during evaluations.,advanced,code,examples/LLM_rag_evals.mdx
uploading RAG evaluation results to Evidently Cloud,"It explains how to upload evaluation results to the Evidently Cloud for tracking and further analysis, enhancing collaboration and data management.",intermediate,text,examples/LLM_rag_evals.mdx
context quality evaluation methods,"The article describes various methods for evaluating context quality in RAG systems, offering code snippets to illustrate their implementation.",advanced,code,examples/LLM_rag_evals.mdx
simulating RAG outputs,"Users learn how to simulate RAG outputs for evaluations without creating an actual RAG app, which is beneficial for preliminary assessments.",beginner,text,examples/LLM_rag_evals.mdx
using LLM for RAG evaluations,It discusses utilizing LLM-based metrics for evaluating adherence to ground truth responses in RAG systems and optimizing prompts for accuracy.,advanced,text,examples/LLM_rag_evals.mdx
regression testing LLM outputs,The article outlines how to conduct regression testing specifically for LLM outputs by comparing responses to ensure consistency and correctness over updates.,beginner,text,examples/LLM_regression_testing.mdx
create toy dataset for LLM,It explains how to build a small Q&A dataset with questions and reference answers for testing LLM responses in a controlled environment.,beginner,code,examples/LLM_regression_testing.mdx
Python example regression testing LLM,The tutorial provides Python code examples to demonstrate each step in the regression testing process for LLM outputs using Evidently Cloud.,beginner,code,examples/LLM_regression_testing.mdx
how to run tests on LLM responses,You can run tests on LLM responses by using the tutorial's structured approach that includes defining evaluation metrics for correctness and style.,intermediate,text,examples/LLM_regression_testing.mdx
Evidently Cloud setup Python,Setting up Evidently Cloud involves installing the package and configuring it with your API token as detailed in the article.,intermediate,code,examples/LLM_regression_testing.mdx
importance of monitoring LLM outputs,The article emphasizes the need to monitor LLM outputs to catch any significant deviations after updates that could affect response quality.,beginner,text,examples/LLM_regression_testing.mdx
evaluating LLM style consistency,You can evaluate the style of LLM responses using a custom LLM judge that checks for similarity in tone and structure compared to a reference answer.,intermediate,code,examples/LLM_regression_testing.mdx
what is LLM-as-a-judge,LLM-as-a-judge refers to using a language model to evaluate the correctness and consistency of outputs against reference standards as explained in the article.,beginner,text,examples/LLM_regression_testing.mdx
get answers from LLM after prompt changes,The tutorial describes how to generate new responses from an LLM by modifying prompts and comparing them against reference answers.,beginner,code,examples/LLM_regression_testing.mdx
define correctness in LLM testing,"Correctness in LLM testing is defined as responses that do not contradict reference answers, and this can be evaluated using specific criteria provided in the article.",intermediate,text,examples/LLM_regression_testing.mdx
create monitoring dashboard for LLM,You can create a monitoring dashboard to visualize test results and trends over time as outlined in the tutorial.,intermediate,code,examples/LLM_regression_testing.mdx
Python libraries for LLM regression testing,The article lists libraries like Evidently and dependencies needed to effectively perform regression testing for LLM outputs using Python.,beginner,code,examples/LLM_regression_testing.mdx
testing LLM output length,"To ensure that LLM outputs do not exceed a certain length, the article shows how to include a length check as part of your test suite.",intermediate,code,examples/LLM_regression_testing.mdx
how to generate reports for LLM tests,"You can generate detailed reports on LLM test results using the Evidently library, which is thoroughly explained in the tutorial.",intermediate,code,examples/LLM_regression_testing.mdx
using tags in LLM test reports,"The article mentions using tags in test reports to track versions or parameters, enhancing the clarity of regression testing results.",advanced,text,examples/LLM_regression_testing.mdx
comparing old and new LLM answers,The article illustrates how to compare old and new answers from an LLM after updates to ensure consistency and correctness in outputs.,beginner,code,examples/LLM_regression_testing.mdx
best practices for LLM regression testing,"The article advises on best practices for regression testing, like maintaining a consistent dataset and defining clear evaluation metrics.",intermediate,text,examples/LLM_regression_testing.mdx
LLM judge template parameters,You can customize LLM judge template parameters as described in the article to fine-tune the evaluation process for outputs.,advanced,text,examples/LLM_regression_testing.mdx
charts to visualize LLM testing results,The article discusses how to create charts and visual aids to better track LLM testing results over different periods using dashboards.,intermediate,code,examples/LLM_regression_testing.mdx
LLM quickstart tutorial,"The article offers a dedicated section for beginners starting with LLM evaluations, providing a quickstart tutorial that focuses on evaluating text output quality.",beginner,code,examples/introduction.mdx
ML tutorial examples,"It lists various ML tutorials that provide end-to-end examples of workflows related to machine learning, including a metrics cookbook for different evaluation scenarios.",beginner,code,examples/introduction.mdx
LLM judge optimization example,"The article includes examples on how to optimize LLM judges using prompt optimization, with specific implementations tailored for different classification tasks.",intermediate,code,examples/introduction.mdx
tracing quickstart,"A quickstart guide is available for tracing, which helps users collect inputs and outputs from their AI applications for evaluation.",beginner,code,examples/introduction.mdx
evaluate LLM with human labels,"A tutorial focuses on how to create and evaluate an LLM judge against human labels, providing practical insights and code snippets.",intermediate,code,examples/introduction.mdx
integrating Evidently with Grafana,"The article explains how to visualize Evidently evaluation metrics with Grafana, facilitating data analysis through integration examples.",intermediate,code,examples/introduction.mdx
different LLM evaluation methods,"It outlines various LLM evaluation methods, detailing a mix of reference-based and reference-free approaches, along with practical code examples.",intermediate,text,examples/introduction.mdx
Evidently Open-source UI tutorial,"There’s a tutorial provided on how to create a workspace and project in the Evidently Open-source UI, guiding users through deployment steps.",beginner,code,examples/introduction.mdx
RAG evaluation metrics guide,"A detailed walkthrough of RAG evaluation metrics is provided, helping users understand how to assess the quality of generative outputs.",intermediate,code,examples/introduction.mdx
advanced LLM evaluation scenarios,"Advanced topics, including adversarial testing for LLMs, are discussed, showing how to conduct scenario-based evaluations for risky topics.",advanced,code,examples/introduction.mdx
sign up for LLM evaluation course,Information on signing up for an applied LLM evaluation course with video tutorials and code examples is included for deeper learning.,beginner,text,examples/introduction.mdx
GitHub actions for Evidently,"It describes how to utilize GitHub actions to run Evidently evaluations as part of a CI/CD workflow, enhancing integration efficiency.",intermediate,code,examples/introduction.mdx
Evidently Cloud v2 features and upgrades,"The article highlights the major improvements in Evidently Cloud v2, such as a redesigned dashboard, enhanced performance, and better LLM evaluation support.",beginner,text,faq/cloud_v2.mdx
Evidently 0.6 API changes,The article highlights the key changes in the Evidently library with the introduction of the new API and the Report object in version 0.6.,beginner,text,faq/migration.mdx
how to implement new Report object in Evidently,You can implement the new Report object by importing it from `evidently.future` and using it to generate evaluations with the new structure.,intermediate,code,faq/migration.mdx
transition from old Evidently API,"The migration guide outlines how to transition from the old API to the new one, highlighting the coexistence of both APIs during the transition period.",beginner,text,faq/migration.mdx
Dataset object in Evidently 0.6,"The guide explains that in the new version, users must explicitly create a `Dataset` object when running reports, enhancing data structure clarity.",intermediate,code,faq/migration.mdx
how to add descriptors in Evidently,"Descriptors can be added to your dataset to perform evaluations, and the process is split into computing and aggregating results.",intermediate,code,faq/migration.mdx
unifying Reports and Tests in Evidently,"The article describes the integration of Reports and Tests, allowing users to view test results alongside their report metrics in a single HTML output.",intermediate,text,faq/migration.mdx
using automated column mapping Evidently,Automated column mapping for type and role is still available and can assist in managing data inputs effectively in the new version.,beginner,text,faq/migration.mdx
Evidently open-source features,"The open-source version of Evidently includes all core evaluation features, such as tracing and reports, but lacks premium features available in the commercial edition.",beginner,text,faq/oss_vs_cloud.mdx
Evidently Cloud benefits,"Evidently Cloud is a fully managed solution that includes automatic updates and security patches, ideal for teams focusing on AI development without infrastructure worries.",beginner,text,faq/oss_vs_cloud.mdx
Evidently OSS vs Cloud comparison,"The article outlines the feature differences between Evidently's open-source and commercial cloud versions, emphasizing core functionality versus premium features.",intermediate,text,faq/oss_vs_cloud.mdx
How to use Evidently library,"The Evidently library can be used in Python to run data evaluations and generate reports, making it suitable for individual data scientists and ML engineers.",beginner,code,faq/oss_vs_cloud.mdx
Deployment options for Evidently,"Users can deploy the Evidently platform using either the open-source version, which requires self-management, or the cloud version, which is fully managed.",intermediate,text,faq/oss_vs_cloud.mdx
Setting up Evidently Enterprise,"Evidently Enterprise offers a self-hosted option for organizations requiring strict data security, along with dedicated support for implementation and maintenance.",advanced,text,faq/oss_vs_cloud.mdx
telemetry data collected by Evidently,"Evidently collects anonymous usage data to understand user interactions, including environment and service usage data, but does not include personal information.",beginner,text,faq/telemetry.mdx
how to disable telemetry in Evidently,"You can disable telemetry by setting the environment variable DO_NOT_TRACK to any value, such as 'export DO_NOT_TRACK=1'.",beginner,code,faq/telemetry.mdx
what does Evidently collect for telemetry,"Evidently collects data like timestamps, OS details, and user actions within the Monitoring UI to improve our tool based on usage patterns.",beginner,text,faq/telemetry.mdx
Evidently telemetry environment data,"The telemetry includes environment data such as OS name and version, Python version, and a unique user ID, which is anonymized.",intermediate,text,faq/telemetry.mdx
anonymity in Evidently telemetry,Evidently ensures that the telemetry data collected does not reveal personal information; data like user ID is anonymized to protect privacy.,intermediate,text,faq/telemetry.mdx
examples of Evidently telemetry logs,"The article provides example logs for various actions like startup, indexing, and adding projects, showcasing how data is structured for analysis.",intermediate,code,faq/telemetry.mdx
Evidently Monitoring UI telemetry settings,Telemetry settings are enabled by default in the Monitoring UI and can be altered via environment variables to track user interactions.,beginner,text,faq/telemetry.mdx
Evidently usage data significance,Collecting usage data helps prioritize new features and improvements based on actual user engagement and environments used. Opting in is encouraged for community benefit.,beginner,text,faq/telemetry.mdx
opt-out of Evidently telemetry,Users can opt-out of telemetry by following specific steps to disable data collection via environment variable settings.,beginner,code,faq/telemetry.mdx
detailed telemetry data in open-source tools,"Open-source tools like Evidently collect telemetry data for improving usability, and the article discusses the importance of this data for future developments.",advanced,text,faq/telemetry.mdx
Evidently AI features,"The article details the comprehensive feature set of Evidently AI, including 100+ built-in evaluations, modular architecture, and integration capabilities.",beginner,text,faq/why_evidently.mdx
open-source Evidently library,"It explains that Evidently is an open-source library with transparency in metrics and a developer-friendly API, available for anyone to use and contribute to.",beginner,text,faq/why_evidently.mdx
AI evaluation tools comparison,"The article compares Evidently with other evaluation tools, emphasizing its ready-to-use evaluations versus the need for custom metrics in alternative solutions.",intermediate,text,faq/why_evidently.mdx
how to implement Evidently in projects,"It provides insights on implementing Evidently in AI projects, outlining its modular setup and easy data handling for evaluations and monitoring.",intermediate,code,faq/why_evidently.mdx
evidently library evaluation metrics examples,"The article details how to use the Evidently open-source Python library, showcasing over 100 evaluation metrics and providing practical examples for users.",beginner,code,introduction.mdx
row level text descriptor example,"The article provides examples of descriptors used for evaluating row-level text, detailing functions like ExactlyMatch, RegExp, and Contains.",beginner,code,metrics/all_descriptors.mdx
LLM evaluations explained,"The article explains LLM evaluations, including how to implement them using various descriptors like LLMEval and ContextRelevance.",beginner,text,metrics/all_descriptors.mdx
using ExactMatch for text comparison,"ExactMatch can be used to check if the contents of two columns are the same, returning a boolean for each input based on the defined columns.",intermediate,code,metrics/all_descriptors.mdx
how to use RegExp in evaluations,"RegExp allows for matching text against a specified regular expression to verify conformity to certain patterns, returning True or False for each input.",intermediate,code,metrics/all_descriptors.mdx
Check for valid JSON format,IsValidJSON checks if a given column contains valid JSON data and returns a boolean for every entry.,beginner,code,metrics/all_descriptors.mdx
determine text sentiment,"Sentiment analysis can be conducted using the Sentiment descriptor, which scores text from negative to positive based on sentiment analysis models.",intermediate,code,metrics/all_descriptors.mdx
how to create custom descriptors,CustomDescriptor allows users to implement their own checks using Python functions that can validate specific datasets as needed.,advanced,code,metrics/all_descriptors.mdx
what is DoesNotContain used for,DoesNotContain checks if specified items are absent from the text and provides a True/False result for each row.,intermediate,code,metrics/all_descriptors.mdx
Parameters for Contains descriptor,Contains requires a list of items to check for their presence within the text and can be configured for case sensitivity.,beginner,code,metrics/all_descriptors.mdx
how to validate SQL queries,"IsValidSQL checks if the column contains valid SQL queries without execution, returning a boolean for each entry.",intermediate,code,metrics/all_descriptors.mdx
using LLM for text evaluation,"LLMEval facilitates scoring of text based on user-defined criteria using a designated provider and model, enabling flexible evaluations.",intermediate,code,metrics/all_descriptors.mdx
text length measurement,"TextLength descriptor measures the length of the text in symbols, returning an absolute number for analysis.",beginner,code,metrics/all_descriptors.mdx
difference between ItemMatch and IncludesWords,"ItemMatch checks if items in a specified list are present based on separate columns, while IncludesWords checks for vocabulary words within the text itself.",intermediate,text,metrics/all_descriptors.mdx
using the NonLetterCharacterPercentage descriptor,This descriptor calculates the percentage of non-letter characters in the text and returns a score on a scale from 0 to 100.,beginner,code,metrics/all_descriptors.mdx
context relevance assessment,"ContextRelevance evaluates if the context is relevant to a provided question using semantic similarity measures, allowing aggregation methods.",advanced,code,metrics/all_descriptors.mdx
parameters for JSONSchemaMatch,"JSONSchemaMatch requires an expected schema to validate JSON objects, with options for exact matching and type validation.",advanced,code,metrics/all_descriptors.mdx
how to detect toxicity in texts,"ToxicityLLMEval utilizes models to detect toxic language in texts, returning labels or scores to evaluate text safety.",advanced,code,metrics/all_descriptors.mdx
how to learn about Core Concepts,The article links to a section on Core Concepts that provides foundational information for understanding evaluations and descriptors.,beginner,text,metrics/all_descriptors.mdx
checking for Out-Of-Vocabulary words,"OOVWordsPercentage computes the percentage of words in the text that are not part of an established vocabulary, returning a score between 0 and 100.",intermediate,code,metrics/all_descriptors.mdx
How to implement CustomColumnsDescriptor,CustomColumnsDescriptor allows users to apply custom checks through a Python function that assesses a specific column in the dataset.,advanced,code,metrics/all_descriptors.mdx
Parameters for CorrectnessLLMEval,CorrectnessLLMEval requires a target output column for comparison to evaluate the accuracy of generated responses from the model.,advanced,code,metrics/all_descriptors.mdx
checking if text ends with a specific suffix,"EndsWith checks if the text concludes with a specified suffix, returning a boolean value for input validation.",beginner,code,metrics/all_descriptors.mdx
Configuration options for WordMatch descriptor,"WordMatch validates if specific vocabulary words are included in the text, allowing for various configurations like case sensitivity and lemmatization.",intermediate,code,metrics/all_descriptors.mdx
how to measure sentence count,"SentenceCount provides a straightforward way to count the number of sentences in the text, returning an absolute numerical value.",beginner,code,metrics/all_descriptors.mdx
detecting PII in responses,"PIILLMEval identifies if texts contain personally identifiable information (PII), returning scores or labels based on findings.",advanced,code,metrics/all_descriptors.mdx
syntax validation with IsValidPython,"IsValidPython checks columns for valid Python code without syntax errors, ensuring code integrity before execution.",advanced,code,metrics/all_descriptors.mdx
dataset-level eval metrics,"The article provides a comprehensive reference for all dataset-level evaluation metrics, detailing their functionality, parameters, and test defaults.",beginner,text,metrics/all_metrics.mdx
how to use TextEvals metric,"TextEvals metric allows users to evaluate text or LLM inputs by specifying descriptors, and it returns a summary of various metrics.",beginner,code,metrics/all_metrics.mdx
difference between MeanValue and MaxValue metrics,"MeanValue computes the average of values in a numerical column, while MaxValue finds the highest value in that column.",intermediate,text,metrics/all_metrics.mdx
example implementation of ValueStats,"ValueStats metric computes various descriptive statistics for a specified column, returning unique counts, missing values, and other relevant info.",intermediate,code,metrics/all_metrics.mdx
parameters for MissingValueCount,"MissingValueCount requires the specification of a column to count missing values, helping assess data completeness.",beginner,text,metrics/all_metrics.mdx
how to calculate accuracy in classification,"Accuracy metric calculates the ratio of correct predictions to total predictions, assessing model performance in classification tasks.",beginner,code,metrics/all_metrics.mdx
test defaults for OutRangeValueCount,OutRangeValueCount defaults are set to fail if any value in the specified range is outside the defined min-max limits.,intermediate,text,metrics/all_metrics.mdx
Column data quality metrics,"The document outlines various metrics for assessing column data quality, such as MissingValueCount and UniqueValueCount.",beginner,text,metrics/all_metrics.mdx
using dataset-level metrics,"Dataset-level metrics like DatasetStats help summarize key statistics about the overall dataset, including row and column counts.",beginner,code,metrics/all_metrics.mdx
conditional tests in metrics,The article explains how to apply conditional tests within metrics using operators like eq (equal) and gt (greater than).,intermediate,text,metrics/all_metrics.mdx
metrics for evaluating regression models,The document provides a list of metrics such as MeanError and RMSE that can be used to evaluate regression model performance.,intermediate,text,metrics/all_metrics.mdx
how to visualize classification metrics,Classification metrics can be visualized using tools like confusion matrices and PR curves to better understand model performance.,advanced,code,metrics/all_metrics.mdx
how to read the metrics table,"The article explains how to interpret the tables listing metrics, including metric names, descriptions, parameters, and test defaults.",beginner,text,metrics/all_metrics.mdx
advanced setup for DataDriftPreset,"DataDriftPreset calculates data drift for specified columns using a chosen drift method, useful for tracking distribution changes.",advanced,code,metrics/all_metrics.mdx
importance of data definition in metrics,Understanding data definition is crucial for correctly mapping column types and applying metrics effectively in evaluation processes.,beginner,text,metrics/all_metrics.mdx
how to implement UniqueValueCount,"To use UniqueValueCount, specify the column of interest, which helps gauge the diversity of values in the dataset.",intermediate,code,metrics/all_metrics.mdx
test defaults for MeanValue,MeanValue has test defaults that determine pass/fail conditions based on comparison results with reference datasets.,intermediate,text,metrics/all_metrics.mdx
what is the purpose of DataSummaryPreset,DataSummaryPreset combines overall dataset statistics and value statistics for a comprehensive overview of data quality.,beginner,text,metrics/all_metrics.mdx
key metrics for multi-class classification,Metrics like PrecisionByLabel and F1ByLabel help evaluate and compare performance across multiple classes in classification tasks.,intermediate,code,metrics/all_metrics.mdx
func usage for ROC AUC,"ROC AUC is crucial for evaluating model performance, determining how well the model separates positive and negative classes.",intermediate,text,metrics/all_metrics.mdx
dataset missing value implications,The article emphasizes the importance of tracking missing values across datasets to ensure data quality and integrity.,beginner,text,metrics/all_metrics.mdx
average cases in Dummy model metrics,Dummy model metrics provide baseline performance expectations which can be used to gauge the effectiveness of more complex models.,intermediate,text,metrics/all_metrics.mdx
NDCG metric definition,Normalized Discounted Cumulative Gain (NDCG) evaluates ranking quality by measuring the usefulness of retrieved items in a ranked list.,advanced,text,metrics/all_metrics.mdx
how to compare model accuracy,"Model accuracy can be compared against dummy model metrics to ensure it outperforms random predictions, proving its effectiveness.",intermediate,code,metrics/all_metrics.mdx
details on ClassificationDummyQuality,"ClassificationDummyQuality summarizes the performance of a baseline dummy model, providing insights on expected accuracy levels.",beginner,text,metrics/all_metrics.mdx
aggregating results with InRangeValueCount,"InRangeValueCount allows for counting how many values fall within a specified range, offering insights into data distribution.",intermediate,code,metrics/all_metrics.mdx
using Count variables for regression,Count variables such as RowCount and ColumnCount help provide basic dataset metrics vital for regression analysis.,beginner,text,metrics/all_metrics.mdx
adjusting probas_threshold in classification,"Adjusting the probas_threshold parameter can alter the sensitivity of classification metrics, affecting model evaluations.",intermediate,text,metrics/all_metrics.mdx
understanding test defaults in metrics,"Test defaults in metrics dictate conditions under which metrics pass or fail, guiding evaluation processes effectively.",beginner,text,metrics/all_metrics.mdx
description of PrecisionTopK metric,"PrecisionTopK calculates the accuracy of the top K items retrieved in a recommendation system, indicating the relevance of results.",intermediate,code,metrics/all_metrics.mdx
implications of ConstantColumnsCount,"ConstantColumnsCount identifies columns with no variability, which can affect model training and predictions.",intermediate,text,metrics/all_metrics.mdx
setting thresholds for F-beta scores,"F-beta scores can be fine-tuned using thresholds to balance precision and recall, adapting the model to specific needs.",advanced,code,metrics/all_metrics.mdx
how to read testing setups for F1Score,Reading and interpreting testing setups for F1Score helps understand how well a model combines precision and recall performance.,intermediate,text,metrics/all_metrics.mdx
relevance of MAPE in regression,"Mean Absolute Percentage Error (MAPE) offers a perspective on averages in regression results, crucial for performance evaluation.",beginner,text,metrics/all_metrics.mdx
how to apply regression metrics in evaluation,"Regression metrics like MAE and RMSE are essential for assessing the quality of predictive models, guiding model tuning and selection.",intermediate,code,metrics/all_metrics.mdx
unique values computation with CategoryCount,"CategoryCount helps in counting occurrences of specified categories within a column, enabling better data analysis.",beginner,code,metrics/all_metrics.mdx
understanding MeanValue vs StdValue,"Comparing MeanValue and StdValue provides insights into both the average and variability of data, crucial for understanding distributions.",intermediate,text,metrics/all_metrics.mdx
using preset evaluation templates,"The article outlines different preset evaluation templates that can be used without setup, including options for text evaluations and data drift detection.",beginner,code,metrics/all_presets.mdx
custom data drift detection methods,"The article explains how to customize data drift detection methods based on column types and volume, allowing for tailored analysis.",beginner,text,metrics/customize_data_drift.mdx
set thresholds for data drift,You can set specific thresholds for each data drift detection method to refine your analysis criteria as described in the article.,beginner,code,metrics/customize_data_drift.mdx
default data drift algorithm,The article outlines the default data drift algorithm which automatically selects methods based on data type and volume.,beginner,text,metrics/customize_data_drift.mdx
override data drift parameters,You can override default drift parameters by passing custom settings to Metrics or Presets for more control over drift detection.,intermediate,code,metrics/customize_data_drift.mdx
how to implement a custom drift method,It provides a guide on implementing a custom drift detection method using a specific function and registering it globally.,advanced,code,metrics/customize_data_drift.mdx
dataset drift share setting,"The article describes how to set the share of drifting columns to detect dataset drift, with examples for customization.",intermediate,code,metrics/customize_data_drift.mdx
data drift detection examples,Includes practical examples of setting drift detection methods and thresholds for both dataset and column levels.,beginner,code,metrics/customize_data_drift.mdx
drift detection for categorical columns,"You can specify drift detection methods for categorical columns, as explained in the article with example code snippets.",intermediate,code,metrics/customize_data_drift.mdx
differences in drift detection methods,The article outlines how different threshold settings affect drift detection outcomes depending on the method used.,advanced,text,metrics/customize_data_drift.mdx
population stability index for drift,It includes the use of the Population Stability Index (PSI) method for detecting drift across dataset columns.,intermediate,code,metrics/customize_data_drift.mdx
drift score definitions,Provides explanations of how different drift scores are calculated based on the detection method applied.,intermediate,text,metrics/customize_data_drift.mdx
multi-column drift detection settings,The article describes how to apply drift detection settings across multiple columns in a dataset effectively.,intermediate,code,metrics/customize_data_drift.mdx
importance of drift thresholds,It highlights the importance of setting appropriate drift thresholds and how they relate to drift detection methods.,beginner,text,metrics/customize_data_drift.mdx
custom functions for data drift,Discusses how to create and implement custom functions for data drift detection alongside provided examples.,advanced,code,metrics/customize_data_drift.mdx
detecting drift in numerical columns,The article explains how to detect drift specifically for numerical columns using various statistical tests.,intermediate,text,metrics/customize_data_drift.mdx
Python code for data drift report,"Offers Python code snippets to create and customize data drift reports for analysis, like setting drift methods and thresholds.",beginner,code,metrics/customize_data_drift.mdx
custom text evaluator example,"The article provides code examples for creating custom text evaluators using `CustomColumnDescriptor` and `CustomDescriptor` in Evidently, allowing users to implement their own evaluation logic.",beginner,code,metrics/customize_descriptor.mdx
how to use CustomColumnDescriptor,"It details how to define a `CustomColumnDescriptor` for evaluating specific columns, including a sample implementation for checking if a column is empty.",intermediate,code,metrics/customize_descriptor.mdx
define custom descriptor in Evidently,"The article explains how to create a `CustomDescriptor` for multi-column evaluations, with examples like checking the exact match between answer columns and transforming data.",advanced,code,metrics/customize_descriptor.mdx
HuggingFace models for text evaluation,"The article details how to score text using HuggingFace models, allowing classification based on various criteria like emotion.",beginner,text,metrics/customize_hf_descriptor.mdx
how to import HuggingFace descriptors,"You can import the HuggingFace models using specific Python code, which is shown in the article under the imports section.",beginner,code,metrics/customize_hf_descriptor.mdx
toy data example with HuggingFace models,The article provides a code snippet for creating toy data to demonstrate using HuggingFace models for evaluations.,beginner,code,metrics/customize_hf_descriptor.mdx
using HuggingFaceToxicity descriptor,The article explains how to use the HuggingFaceToxicity descriptor to classify text by toxicity levels in a provided code example.,intermediate,code,metrics/customize_hf_descriptor.mdx
custom checks with HuggingFace models,"You can create custom evaluations in HuggingFace by defining checks directly as a Python function, as discussed in the article.",intermediate,code,metrics/customize_hf_descriptor.mdx
available HuggingFace models list,The article points to a list of available HuggingFace models that can be used with the descriptor for various evaluations.,beginner,text,metrics/customize_hf_descriptor.mdx
evaluate curiosity in text using HuggingFace,"You can evaluate expressions of curiosity in texts by using the HuggingFace descriptor, as shown in a provided example code snippet.",intermediate,code,metrics/customize_hf_descriptor.mdx
emotion classification model example,The article outlines how to implement an emotion classification model using HuggingFace with specific parameters for labels.,advanced,code,metrics/customize_hf_descriptor.mdx
integrating HuggingFace with other models,"You can use HuggingFace's general interface to integrate other models, which the article describes with examples.",advanced,code,metrics/customize_hf_descriptor.mdx
using HuggingFace for zero-shot classification,The article describes how to employ HuggingFace for zero-shot classification enabling the classification of text without pre-trained topics.,advanced,code,metrics/customize_hf_descriptor.mdx
LLM judges setup tutorial,"The article explains how to configure LLM judges using prompt-based evaluators in Python, detailing necessary libraries and setup.",beginner,text,metrics/customize_llm_judge.mdx
built-in LLM evaluators,It covers the available built-in evaluators and how to use them in the evaluation process with examples for toxicity and context quality.,beginner,text,metrics/customize_llm_judge.mdx
how to use ToxicityLLMEval,Instructions are provided on applying the ToxicityLLMEval descriptor to evaluate responses for toxicity in text columns.,beginner,code,metrics/customize_llm_judge.mdx
creating custom LLM evaluators,"The article shows how to create custom LLM evaluators using provided templates, allowing specific grading logic and categories.",intermediate,code,metrics/customize_llm_judge.mdx
examples of LLM evaluation templates,You can find examples of LLM evaluation templates for binary and multi-class classifications that can be adapted to specific needs.,intermediate,code,metrics/customize_llm_judge.mdx
multi-column LLM evaluation setup,"The guide discusses how to set up multi-column evaluations using LLM judges, including sample code to implement this.",intermediate,code,metrics/customize_llm_judge.mdx
OpenAI API key configuration,It provides steps for configuring the OpenAI API key as an environment variable to access built-in evaluators.,beginner,code,metrics/customize_llm_judge.mdx
custom parameters for LLM judges,"The article explains how to customize parameters when deploying LLM judges, refining their responses according to specific criteria.",intermediate,code,metrics/customize_llm_judge.mdx
using alternative LLM providers,"Instructions are given for switching to alternative LLM providers like Anthropic, including necessary API configurations.",advanced,code,metrics/customize_llm_judge.mdx
evaluate response context in LLM,The article describes how to evaluate the context quality of responses using the ContextQualityLLMEval.,beginner,code,metrics/customize_llm_judge.mdx
how to install Evidently library,"A brief setup guide is provided on installing the Evidently library, essential for managing LLM judges and descriptors.",beginner,code,metrics/customize_llm_judge.mdx
common LLM evaluation descriptors,"The article details various LLM evaluation descriptors, helping readers understand their specific usage and implementation.",intermediate,text,metrics/customize_llm_judge.mdx
LLM judge parameter explanation,A comprehensive table of parameters for LLMEval and BinaryClassificationPromptTemplate explaining their purpose and options is provided.,intermediate,text,metrics/customize_llm_judge.mdx
evaluate multiple classes in LLM,This section elaborates on implementing multi-class evaluations using defined grading rubrics within LLM judges.,advanced,code,metrics/customize_llm_judge.mdx
installing pandas for data handling,"The article touches on installing pandas, which is necessary for data manipulation when using the LLM judges.",beginner,code,metrics/customize_llm_judge.mdx
configuring a multi-class classification prompt,Instructions are provided on configuring multi-class classification prompts and applying them to evaluations within LLM judges.,intermediate,code,metrics/customize_llm_judge.mdx
best practices for API key safety,The article emphasizes best practices for safely managing your OpenAI API key when using LLM judges in production.,intermediate,text,metrics/customize_llm_judge.mdx
LLM evaluations in Python examples,"The article includes practical examples of LLM evaluations in Python, making it accessible for developers to implement.",beginner,code,metrics/customize_llm_judge.mdx
results interpretation from LLM judges,Guidance is given on how to interpret the results returned by LLM judges regarding text evaluations and descriptors.,intermediate,text,metrics/customize_llm_judge.mdx
overview of Evidently pipeline,An overview of how the Evidently pipeline integrates LLM evaluations into data analytics is provided in the article.,intermediate,text,metrics/customize_llm_judge.mdx
customizing LLM evaluation logic,The article teaches how to customize evaluation logic using templates to achieve specific grading results within your LLM setup.,advanced,code,metrics/customize_llm_judge.mdx
LLM evaluators troubleshooting tips,It includes troubleshooting tips for common pitfalls when setting up LLM judges and applying evaluations.,advanced,text,metrics/customize_llm_judge.mdx
understanding evaluation output formats,"The article covers different output formats available from LLM evaluations, helping clarify what to expect from results.",intermediate,text,metrics/customize_llm_judge.mdx
adapting templates for specific criteria,Instructions for adapting existing LLM evaluation templates for specific criteria relevant to your application are provided.,advanced,code,metrics/customize_llm_judge.mdx
creating custom metrics in evidently,"This article provides a detailed guide on building custom metrics and tests for dataset evaluations in Evidently, focusing on implementing the metric calculation method and optional visualizations.",beginner,code,metrics/customize_metric.mdx
evidently custom metric example,"An example of implementing a custom metric, `MyMaxMetric`, for calculating the maximum value in a column is included, demonstrating the required methods and code structure.",intermediate,code,metrics/customize_metric.mdx
how to add visualizations to metrics evidently,"The article explains how to create custom visualizations for metrics using Plotly, detailing steps to enhance the visual output of the metrics implemented.",intermediate,text,metrics/customize_metric.mdx
default tests for custom metrics,"It discusses how to define optional default test conditions for custom metrics, specifying checks applied during the evaluation process without custom conditions.",advanced,text,metrics/customize_metric.mdx
classification metrics explained,"The article details standard classification metrics like Accuracy, Precision, Recall, and F1-score for evaluating model performance.",beginner,text,metrics/explainer_classification.mdx
how to calculate precision recall,Calculating precision and recall is outlined along with their significance in assessing model performance in the article.,beginner,code,metrics/explainer_classification.mdx
ROC AUC curve summary,"The article explains the ROC AUC curve, describing how it measures the trade-off between true positive and false positive rates in classification tasks.",intermediate,text,metrics/explainer_classification.mdx
confusion matrix visualization,"It provides visual representations of confusion matrices, helping analyze classification errors by showing the predicted versus actual classifications.",beginner,code,metrics/explainer_classification.mdx
examples of quality metrics by class,The section illustrates how to compute and interpret quality metrics for individual classes in multi-class classification problems.,intermediate,code,metrics/explainer_classification.mdx
importance of precision-recall curve,"The article covers the precision-recall curve, discussing its role in evaluating the balance between precision and recall at different thresholds.",beginner,text,metrics/explainer_classification.mdx
improving model using class separation quality,"Strategies for improving model performance based on class separation quality visualizations are discussed, emphasizing the choice of probability thresholds.",advanced,code,metrics/explainer_classification.mdx
analyzing prediction distributions,"Methods for analyzing the distribution of predicted probabilities to understand model predictions are detailed in the article, helping to optimize performance.",intermediate,text,metrics/explainer_classification.mdx
data quality overview widget,"The article explains the summary widget which provides an overview of the dataset, highlighting missing features and constants, specifically showing shares of almost empty and constant features.",beginner,text,metrics/explainer_data_stats.mdx
visualizing features in datasets,"It describes how the features widget generates visualizations for each feature type, including statistical summaries and feature distributions, tailored for categorical, numerical, and datetime features.",beginner,code,metrics/explainer_data_stats.mdx
summary of missing features in data,The article covers how the summary widget specifically addresses missing or empty features and displays general information about the dataset's quality.,beginner,text,metrics/explainer_data_stats.mdx
feature correlation analysis,There is a section on the correlation widget which illustrates how to assess correlations between different features and highlights significant relationships through summary tables and heatmaps.,intermediate,code,metrics/explainer_data_stats.mdx
categorical feature statistics,"The article provides examples of visualizing categorical features, including tables and graphs that summarize their distributions and interactions with target variables.",intermediate,code,metrics/explainer_data_stats.mdx
how to identify almost constant features,"It explains the method to detect features that are almost constant or almost empty, specifically those with 95% or more missing values.",beginner,text,metrics/explainer_data_stats.mdx
Cramer's V correlation method,"The article discusses the use of Cramer's V for assessing correlations between categorical features, along with other correlation measures for numerical data.",advanced,text,metrics/explainer_data_stats.mdx
data drift detection algorithm,The article discusses Evidently's default algorithm for detecting data drift by comparing distributions of reference and current datasets across various features.,beginner,text,metrics/explainer_drift.mdx
how to detect data drift in datasets,Evidently uses statistical tests to compare datasets and returns results on whether drift is detected based on various column types and statistics.,beginner,code,metrics/explainer_drift.mdx
required datasets for data drift,"To evaluate drift, you need two datasets: a reference and a current dataset, ensuring columns are non-empty before testing.",beginner,text,metrics/explainer_drift.mdx
how to handle empty values in data drift,"The article explains that empty or infinite values are filtered out during calculations, and drift tests don't react to changes in the number of empty values.",intermediate,text,metrics/explainer_drift.mdx
examples of drift detection methods,"Evidently offers multiple methods like Kolmogorov-Smirnov, chi-squared, and Jensen-Shannon divergence for detecting different types of drift based on data size and column type.",intermediate,code,metrics/explainer_drift.mdx
drift score and thresholds explained,"The drift score represents the statistical significance of drift detection, with different thresholds and metrics used depending on data size and type to assess drift.",intermediate,text,metrics/explainer_drift.mdx
detecting dataset-level drift,You can declare dataset-level drift if a certain percentage of features are detected as drifted by using Evidently's presets like DatasetDriftPreset.,advanced,code,metrics/explainer_drift.mdx
ROC AUC for text data drift detection,"The article explains how text data drift is detected using a domain classifier, assessing the ROC AUC score against set thresholds to determine drift.",advanced,code,metrics/explainer_drift.mdx
Recall at K definition for recommender systems,"The article explains that Recall at K measures the proportion of relevant items retrieved within the top K results, evaluating the effectiveness of recommender systems.",beginner,text,metrics/explainer_recsys.mdx
How to compute precision at K in ranking,"Precision at K is computed by determining the share of relevant items within the top K results, as discussed in the article, which includes methods for calculating it by user and overall.",beginner,code,metrics/explainer_recsys.mdx
F Beta score calculation for ranking,"The article details how to calculate the F Beta score, which combines precision and recall into a single metric, emphasizing its function in evaluating system performance.",intermediate,code,metrics/explainer_recsys.mdx
Understanding MAP metric in recommendations,Mean Average Precision (MAP) assesses how well a system suggests relevant items within the top-K results while penalizing the positions of relevant items lower in the ranking.,intermediate,text,metrics/explainer_recsys.mdx
What is NDCG and its importance,"NDCG (Normalized Discounted Cumulative Gain) measures the ranking quality of a system, highlighting its effectiveness in placing relevant items at the top of results, explained in depth in the article.",beginner,text,metrics/explainer_recsys.mdx
Implementing MAR in recommendation systems,"The article outlines how to compute Mean Average Recall (MAR), which evaluates the system's ability to retrieve all relevant items, averaged by all relevant positions in the ranking.",intermediate,code,metrics/explainer_recsys.mdx
Hit Rate in user recommendations,"Hit Rate indicates the fraction of users for whom at least one relevant item is included in the top K recommendations, emphasizing its relevance in measuring user satisfaction.",beginner,text,metrics/explainer_recsys.mdx
Mean Reciprocal Rank (MRR) calculation steps,"MRR is calculated by finding the position of the first relevant item for each user and averaging the reciprocal ranks, a method discussed in the article for evaluating ranking quality.",intermediate,code,metrics/explainer_recsys.mdx
Difference between precision and recall,"The article distinguishes between precision, which focuses on relevant items in the top K, and recall, which measures the retrieval of all relevant items, highlighting their complementary roles in evaluations.",advanced,text,metrics/explainer_recsys.mdx
Entropy score distribution in recommender systems,"Entropy measures the unpredictability in score distribution for recommended items, as explained in the article, and is computed using softmax transformation and KL divergence.",advanced,code,metrics/explainer_recsys.mdx
regression metrics overview,"The article provides an overview of open-source regression metrics such as Mean Error, Mean Absolute Error, and Mean Absolute Percentage Error.",beginner,text,metrics/explainer_regression.mdx
Mean Absolute Percentage Error example,It explains how to calculate Mean Absolute Percentage Error (MAPE) and provides examples for understanding its significance in model evaluation.,beginner,code,metrics/explainer_regression.mdx
how to interpret predicted vs actual scatter plot,The article discusses the predicted vs actual scatter plot as a tool for visually assessing model accuracy and identifying potential areas for improvement.,intermediate,text,metrics/explainer_regression.mdx
error distribution visualization in regression,"It covers how error distribution visualizations can reveal the spread and concentration of model errors, aiding in performance analysis.",intermediate,code,metrics/explainer_regression.mdx
calculate mean absolute error in Python,The article includes Python code snippets to demonstrate how to calculate the Mean Absolute Error (MAE) for regression modeling.,intermediate,code,metrics/explainer_regression.mdx
understanding Mean Error per group,"It explains the Mean Error metrics calculated per segment of predictions, specifically focusing on overestimation and underestimation groups.",advanced,text,metrics/explainer_regression.mdx
error normality quantile plot,The article details how to create and interpret a quantile-quantile (Q-Q) plot to assess the normality of model errors.,advanced,code,metrics/explainer_regression.mdx
absolute percentage error calculation,It provides a step-by-step guide on calculating the Absolute Percentage Error (APE) and its importance in regression analysis.,beginner,code,metrics/explainer_regression.mdx
visualizing predictions per feature,The article outlines methods for visualizing predicted vs actual values per feature to analyze feature impact on model performance.,intermediate,text,metrics/explainer_regression.mdx
classification preset implementation example,The article provides code examples on how to implement the `ClassificationPreset` using the Report class to evaluate performance on datasets.,beginner,code,metrics/preset_classification.mdx
how to visualize classification metrics,"It explains various metrics and visualizations available in the `ClassificationPreset`, such as confusion matrices and ROC curves, to help assess classification model performance.",intermediate,text,metrics/preset_classification.mdx
customizing classification report conditions,"The article describes how to modify test conditions and report composition, allowing users to tailor the evaluation approach based on specific needs or data characteristics.",advanced,code,metrics/preset_classification.mdx
data drift preset overview,"The article provides an overview of the Data Drift Preset, explaining how it evaluates shifts in data distribution between current and reference datasets.",beginner,text,metrics/preset_data_drift.mdx
how to run data drift report,"To run a Data Drift report, you can use the Report class with DataDriftPreset and compare the current data with reference data in Python.",beginner,code,metrics/preset_data_drift.mdx
data drift detection methods,"The article discusses various drift detection methods available in the Data Drift Preset, including PSI, K-L divergence, and others based on the column type.",intermediate,text,metrics/preset_data_drift.mdx
customize data drift report in Python,"You can customize the Data Drift report by selecting specific columns, changing drift detection methods, adjusting thresholds, and adding additional metrics.",intermediate,code,metrics/preset_data_drift.mdx
best practices for monitoring data drift,"The article suggests monitoring for data drift to maintain model performance, especially in scenarios where ground truth labels are absent, via proxy metrics.",advanced,text,metrics/preset_data_drift.mdx
DataSummaryPreset usage example,The article provides a Python code snippet demonstrating how to run a `DataSummaryPreset` on a dataset for generating descriptive statistics and visualizations.,beginner,code,metrics/preset_data_summary.mdx
How to compare datasets with DataSummaryPreset,"You can pass two datasets to the `DataSummaryPreset` for a side-by-side comparison of descriptive statistics and data quality tests, detailed in the article.",intermediate,text,metrics/preset_data_summary.mdx
Customizing reports in DataSummaryPreset,"The document outlines various report customization options such as selecting specific columns, adding metrics, and modifying test conditions for tailored insights.",advanced,code,metrics/preset_data_summary.mdx
recommender systems overview,"The article provides an overview of the Recommender Systems Preset available in the Evidently API, explaining how it evaluates the quality of recommendations using various metrics and tests.",beginner,text,metrics/preset_recsys.mdx
setting up RecsysPreset with Python,"Instructions for setting up the `RecsysPreset` using Python are included, demonstrating how to run the report and evaluate recommendations with specific parameters like `k`.",beginner,code,metrics/preset_recsys.mdx
customizing report conditions Recsys,"The article explains how to customize report conditions in the Recommender Systems Preset, allowing users to modify test conditions and add metrics for deeper analysis.",intermediate,code,metrics/preset_recsys.mdx
regression quality preset overview,"The article explains the Regression Quality Preset, detailing its purpose in evaluating and visualizing regression performance metrics like MAE, MSE, and RMSE.",beginner,text,metrics/preset_regression.mdx
how to run report with regression preset,It provides a code example for running a report with the RegressionPreset on a current dataset using Python.,beginner,code,metrics/preset_regression.mdx
customize regression report settings,"The article outlines ways to customize regression report settings, such as changing test conditions and adding additional metrics to the report.",intermediate,code,metrics/preset_regression.mdx
Text Evals Preset usage example,The article provides an example of how to utilize the Text Evals Preset to run reports on a dataset using computed descriptors for text evaluations.,beginner,code,metrics/preset_text_evals.mdx
How to compare two datasets in Text Evals,"You can compare two datasets in Text Evals by passing them to the report runner, which allows for side-by-side evaluation results with relevant statistics.",intermediate,code,metrics/preset_text_evals.mdx
Understanding descriptors in Text Evals,Descriptors are crucial for the Text Evals Preset as they summarize the evaluations of text data; detailed explanations on how to compute them are provided in the article.,beginner,text,metrics/preset_text_evals.mdx
LLM evaluation guide,"This article offers a comprehensive guide on how to evaluate text outputs from LLMs, including setup and running evaluations in Python using Evidently.",beginner,text,quickstart_llm.mdx
install Evidently library,Instructions for installing the Evidently Python library are provided to help users set up their environment for LLM evaluations.,beginner,code,quickstart_llm.mdx
set up Evidently Cloud,The article explains how to set up Evidently Cloud for those who want a fully online experience for LLM evaluations.,beginner,code,quickstart_llm.mdx
create project in Evidently,"Step-by-step guidance is given on how to create a project in Evidently, which is essential for organizing your evaluations.",intermediate,code,quickstart_llm.mdx
prepare dataset for LLM evaluation,"The article demonstrates how to prepare a dataset for evaluating LLM outputs, using a toy demo example to illustrate the process.",beginner,code,quickstart_llm.mdx
run LLM evaluation with descriptors,Users are instructed on how to run evaluations using descriptors like sentiment and text length to analyze LLM responses effectively.,intermediate,code,quickstart_llm.mdx
generate evaluation reports in Python,"Instructions are included on generating evaluation reports in Python, which summarize the evaluation results for analysis.",intermediate,code,quickstart_llm.mdx
add test conditions for LLM results,The article discusses how to incorporate test conditions into LLM evaluations to assess outputs against specific criteria.,advanced,code,quickstart_llm.mdx
custom LLM judges for evaluations,It explains how to create custom LLM judges for evaluations using criteria tailored to specific applications or domains.,advanced,code,quickstart_llm.mdx
data drift evaluation with Evidently,The article explains how to evaluate data drift using Evidently by comparing production and reference datasets to detect changes in data distributions.,beginner,code,quickstart_ml.mdx
Evidently setup for ML checks,Includes steps for setting up Evidently Cloud and installing necessary libraries to perform data evaluations in machine learning.,beginner,code,quickstart_ml.mdx
preparing datasets for data drift,"Discusses how to prepare toy datasets for evaluation, including splitting data and defining the schema for analysis.",intermediate,code,quickstart_ml.mdx
Evidently dashboard tracking,Explains how to create dashboards to monitor data drift over time and visualize metrics related to model performance.,intermediate,code,quickstart_ml.mdx
checking prediction accuracy with Evidently,"Details how to evaluate prediction quality and monitor for changes using Evidently's tools, including examples of code implementation.",advanced,code,quickstart_ml.mdx
concept of data drift in ML,"The article introduces the concept of data drift, how it affects machine learning models, and the importance of evaluating it regularly.",beginner,text,quickstart_ml.mdx
setup tracing for LLM app,"The article provides a step-by-step guide on how to set up tracing for an LLM application, including installation and configuration instructions.",beginner,code,quickstart_tracing.mdx
Evidently Cloud overview,It discusses how to utilize the Evidently Cloud platform for viewing and managing traces collected from LLM apps.,beginner,text,quickstart_tracing.mdx
trace event implementation example,The article includes code examples on how to create trace events using the Tracely library to capture inputs and outputs from an LLM.,intermediate,code,quickstart_tracing.mdx
installing Evidently and Tracely,It explains the commands required to install the Evidently and Tracely libraries necessary for tracing in LLM applications.,beginner,code,quickstart_tracing.mdx
advanced tracing configurations,"For advanced users, the article provides specific configurations for initializing tracing and evaluating trace data, making it suitable for customized LLM applications.",advanced,text,quickstart_tracing.mdx
adversarial testing dataset creation,"The article outlines the steps to create an adversarial test dataset in Evidently Cloud, including project setup, scenario selection, and dataset configuration.",beginner,code,synthetic-data/adversarial_data.mdx
automate adversarial test generation Evidently,"The article explains how to automate the generation of adversarial tests based on predefined risk categories using Evidently Cloud, enhancing AI model robustness.",intermediate,text,synthetic-data/adversarial_data.mdx
generate synthetic test inputs for AI,The article explains how to generate synthetic test inputs by defining specific scenarios and describing the types of queries needed for testing AI systems.,beginner,code,synthetic-data/input_data.mdx
generate synthetic test data examples,"The article explains how to generate synthetic test inputs and outputs for evaluating AI systems, including examples for different testing scenarios.",beginner,code,synthetic-data/introduction.mdx
create RAG evaluation dataset steps,"The article outlines a process to create a RAG evaluation dataset using the Evidently UI, which involves starting a project, uploading your knowledge base, generating test cases, and saving the dataset.",beginner,code,synthetic-data/rag_data.mdx
when to use synthetic data for AI testing,"The article highlights scenarios where synthetic data is essential, such as starting without real data, scaling datasets, and testing specific edge cases.",beginner,text,synthetic-data/why_synthetic.mdx
generate synthetic data examples for LLM evaluation,"The article explains how synthetic data can be generated to create structured test cases, especially for complex AI systems like RAG, providing variations and filling gaps in existing datasets.",intermediate,code,synthetic-data/why_synthetic.mdx
